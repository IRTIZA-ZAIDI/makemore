{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "be7dbbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b5c95cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in all the words\n",
    "words = open(\"names.txt\", \"r\").read().splitlines()\n",
    "\n",
    "# build the vocabulary of characters and mappings to/from integers\n",
    "chars = sorted(list(set(\"\".join(words))))\n",
    "stoi = {s: i + 1 for i, s in enumerate(chars)}\n",
    "stoi[\".\"] = 0\n",
    "itos = {i: s for s, i in stoi.items()}\n",
    "vocab_size = len(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fb628718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3]) torch.Size([182625])\n",
      "torch.Size([22655, 3]) torch.Size([22655])\n",
      "torch.Size([22866, 3]) torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "# build the dataset\n",
    "block_size = (\n",
    "    3  # context length: how many characters do we take to predict the next one?\n",
    ")\n",
    "\n",
    "\n",
    "def build_dataset(words):\n",
    "    X, Y = [], []\n",
    "\n",
    "    for w in words:\n",
    "        context = [0] * block_size\n",
    "        for ch in w + \".\":\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix]  # crop and append\n",
    "\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    print(X.shape, Y.shape)\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "\n",
    "n1 = int(0.8 * len(words))\n",
    "n2 = int(0.9 * len(words))\n",
    "\n",
    "Xtr, Ytr = build_dataset(words[:n1])  # 80%\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])  # 10%\n",
    "Xte, Yte = build_dataset(words[n2:])  # 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ce194583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function we will use to compare manual grad with pytorch grad\n",
    "def cmp(s, dt, t):\n",
    "    ex = torch.all(dt == t.grad).item()\n",
    "    app = torch.allclose(dt, t.grad)\n",
    "    maxdiff = (dt - t.grad).abs().max().item()\n",
    "    print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "92803c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4137\n"
     ]
    }
   ],
   "source": [
    "n_embd = 10  # the dimensionality of the character embedding vectors\n",
    "n_hidden = 64  # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)  # for reproducibility\n",
    "C = torch.randn((vocab_size, n_embd), generator=g)\n",
    "\n",
    "# Layer 1\n",
    "W1 = (\n",
    "    torch.randn((n_embd * block_size, n_hidden), generator=g)\n",
    "    * (5 / 3)\n",
    "    / ((n_embd * block_size) ** 0.5)\n",
    ")\n",
    "b1 = (\n",
    "    torch.randn(n_hidden, generator=g) * 0.1\n",
    ")  # using b1 just for fun, it's useless because of batchnorm\n",
    "\n",
    "# Layer 2\n",
    "W2 = torch.randn((n_hidden, vocab_size), generator=g) * 0.1\n",
    "b2 = torch.randn(vocab_size, generator=g) * 0.1\n",
    "\n",
    "# BatchNorm parameters\n",
    "bngain = torch.randn((1, n_hidden)) * 0.1 + 1.0\n",
    "bnbias = torch.randn((1, n_hidden)) * 0.1\n",
    "\n",
    "# Note: I am initializing many of these parameters in non-standard ways\n",
    "# because sometimes initializing with e.g. all zeros could mask an incorrect\n",
    "# implementation of the backward pass.\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters))  # number of parameters in total\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8cea4e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "n = batch_size  # a shorter variable also, for convenience\n",
    "\n",
    "# construct a minibatch\n",
    "ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "Xb, Yb = Xtr[ix], Ytr[ix]  # batch X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "634e74b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.3540, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forward pass, \"chunkated\" into smaller steps that are possible to backward one at a time\n",
    "\n",
    "emb = C[Xb]  # embed the characters into vectors\n",
    "embcat = emb.view(emb.shape[0], -1)  # concatenate the vectors\n",
    "\n",
    "# Linear layer 1\n",
    "hprebn = embcat @ W1 + b1  # hidden layer pre-activation\n",
    "\n",
    "# BatchNorm layer\n",
    "bnmeani = 1 / n * hprebn.sum(0, keepdim=True)\n",
    "bndiff = hprebn - bnmeani\n",
    "bndiff2 = bndiff**2\n",
    "bnvar = (\n",
    "    1 / (n - 1) * (bndiff2).sum(0, keepdim=True)\n",
    ")  # note: Bessel's correction (dividing by n-1, not n)\n",
    "bnvar_inv = (bnvar + 1e-5) ** -0.5\n",
    "bnraw = bndiff * bnvar_inv\n",
    "hpreact = bngain * bnraw + bnbias\n",
    "\n",
    "# Non-linearity\n",
    "h = torch.tanh(hpreact)  # hidden layer\n",
    "\n",
    "# Linear layer 2\n",
    "logits = h @ W2 + b2  # output layer\n",
    "\n",
    "# cross entropy loss (same as F.cross_entropy(logits, Yb))\n",
    "logit_maxes = logits.max(1, keepdim=True).values\n",
    "norm_logits = logits - logit_maxes  # subtract max for numerical stability\n",
    "counts = norm_logits.exp()\n",
    "counts_sum = counts.sum(1, keepdims=True)\n",
    "counts_sum_inv = (\n",
    "    counts_sum**-1\n",
    ")  # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
    "probs = counts * counts_sum_inv\n",
    "logprobs = probs.log()\n",
    "loss = -logprobs[range(n), Yb].mean()\n",
    "\n",
    "# PyTorch backward pass\n",
    "for p in parameters:\n",
    "    p.grad = None\n",
    "\n",
    "for t in [\n",
    "    logprobs,\n",
    "    probs,\n",
    "    counts,\n",
    "    counts_sum,\n",
    "    counts_sum_inv,\n",
    "    norm_logits,\n",
    "    logit_maxes,\n",
    "    logits,\n",
    "    h,\n",
    "    hpreact,\n",
    "    bnraw,\n",
    "    bnvar_inv,\n",
    "    bnvar,\n",
    "    bndiff2,\n",
    "    bndiff,\n",
    "    hprebn,\n",
    "    bnmeani,\n",
    "    embcat,\n",
    "    emb,\n",
    "]:\n",
    "    t.retain_grad()\n",
    "\n",
    "loss.backward()\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600347dd",
   "metadata": {},
   "source": [
    "# Excercise 1 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "60648594",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 8, 14, 15, 22,  0, 19,  9, 14,  5,  1, 20,  3,  8, 14, 12,  0, 11,  0,\n",
       "        26,  9, 25,  0,  1,  1,  7, 18,  9,  3,  5,  9,  0, 18])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1ed14f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logprobs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cdc65560",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-4.0657, -2.9595, -3.6745, -3.2973, -4.1389, -3.5611, -3.1891, -3.9400,\n",
       "        -3.1708, -4.3764, -3.0727, -1.7421, -2.8173, -2.9052, -3.0742, -3.2179,\n",
       "        -3.7922, -3.1004, -3.5414, -3.4171, -2.8918, -3.0620, -4.3925, -4.1390,\n",
       "        -3.4675, -2.7963, -3.0630, -3.9547, -2.7008, -3.4675, -3.3067, -3.0310],\n",
       "       grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logprobs[range(n), Yb]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57126f7",
   "metadata": {},
   "source": [
    "Cross-entropy loss:\n",
    "\n",
    "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi mathvariant=\"script\">L</mi><mo>=</mo><mo>−</mo><munder><mo>∑</mo><mi>k</mi></munder><msub><mi>y</mi><mi>k</mi></msub><mi>log</mi><mo>⁡</mo><mo stretchy=\"false\">(</mo><msub><mi>p</mi><mi>k</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\mathcal{L} = - \\sum_{k} y_k \\log(p_k)</annotation></semantics></math>\n",
    "\n",
    "Because y is one-hot, only one term survives:\n",
    "\n",
    "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi mathvariant=\"script\">L</mi><mo>=</mo><mo>−</mo><mi>log</mi><mo>⁡</mo><mo stretchy=\"false\">(</mo><msub><mi>p</mi><mtext>true</mtext></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\mathcal{L} = - \\log(p_{\\text{true}})</annotation></semantics></math>\n",
    "\n",
    "“the model’s predicted probability assigned to the true class”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea9b710",
   "metadata": {},
   "source": [
    "### dloss / dlogprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b8fac79e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logprobs        | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# dloss / dlogprobs\n",
    "# dlogprobs = ??\n",
    "# loss = -logprobs[range(n), Yb].mean()\n",
    "# loss = -(a + b + c + ..) / n\n",
    "# loss = -a/3 -b/3 -c/3 + ...\n",
    "# dloss/ da = -1/3, dloss/ db = -1/3\n",
    "# dloss/ dn = -1/n\n",
    "\n",
    "dlogprobs = torch.zeros_like(logprobs) # create 0 value tensors in shape of logprobs\n",
    "dlogprobs[range(n), Yb] = -1.0 / n\n",
    "\n",
    "# check\n",
    "cmp('dlogprobs', dlogprobs, logprobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2867b4c0",
   "metadata": {},
   "source": [
    "### dloss/dprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dd80bd4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dprobs          | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# dprobs = ??\n",
    "# logprobs = probs.log()\n",
    "# d/dx log(x) = 1/x\n",
    "# dloss/dprobs = dloss / dlogprobs * dlogprobs / dprobs\n",
    "# dlogprobs / dprobs = 1 / probs\n",
    "\n",
    "dprobs = (1.0 / probs) * dlogprobs\n",
    "# intuitively:\n",
    "# if a correct prediction has high prob, 1/prob is just 1 and it will just pass through for dlogprobs\n",
    "# however if a correct prediction has low prob,  1/prob is high, bossting the gradients\n",
    "\n",
    "\n",
    "# check\n",
    "cmp(\"dprobs\", dprobs, probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24776932",
   "metadata": {},
   "source": [
    "### dloss / dcounts_sum_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b0a371af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dprobs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b6b59364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 27]), torch.Size([32, 1]))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts.shape, counts_sum_inv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a449f8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# c = a * b, but with tensors:\n",
    "# a[3x3] * b[3,1] ---->\n",
    "# a11*b1  a12*b1  a13*b1\n",
    "# a21*b2  a22*b2  a23*b2\n",
    "# a31*b3  a32*b3  a33*b3\n",
    "# c[3x3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "de991e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dcounts_sum_inv | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# probs = counts * counts_sum_inv\n",
    "# softmax step exp(n_x)/sum of all exp(n)\n",
    "\n",
    "# dloss/dcounts_sum_inv = dloss/dprobs * dprobs/dcounts_sum_inv\n",
    "# dprobs/dcounts_sum_inv = ??\n",
    "# [ probs = counts * counts_sum_inv ] * d/ d/dcounts\n",
    "# dprobs/dcounts_sum_inv = counts\n",
    "# hence\n",
    "# dloss/dcounts_sum_inv = dloss/dprobs * counts\n",
    "# REMEBER THAT SINCE THERE IS BROADCASTING, THERE ARE 2 OPERATIONS : REPLICATION & MULTIPLICATION\n",
    "# counts_sum_inv.shape == torch.Size([32, 1])\n",
    "# WE JUST HAVE TO SUM FOR ALL THE GRADIENTS CAUSED BY THE MULTIPLICATION OF EACH BROADCASTED ELEMENT\n",
    "# dprobs.shape == torch.Size([32, 27]) sum across rows\n",
    "# then shape would match !!!\n",
    "dcounts_sum_inv = (counts * dprobs).sum(dim=1, keepdim=True)\n",
    "\n",
    "# check\n",
    "cmp(\"dcounts_sum_inv\", dcounts_sum_inv, counts_sum_inv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8575a5",
   "metadata": {},
   "source": [
    "### dloss / dcounts part1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "67447f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dcounts         | exact: False | approximate: False | maxdiff: 0.005473533645272255\n"
     ]
    }
   ],
   "source": [
    "# probs = counts * counts_sum_inv\n",
    "# softmax step exp(n_x)/sum of all exp(n)\n",
    "\n",
    "# dloss/dcounts = dloss/dprobs * dprobs/dcounts\n",
    "# dprobs/counts = ??\n",
    "# [ probs = counts * counts_sum_inv ] * d/ d/dcounts\n",
    "# dprobs/dcounts = counts_sum_inv\n",
    "# hence\n",
    "# dloss/dcounts_sum_inv = dloss/dprobs * counts_sum_inv\n",
    "# REMEBER THAT SINCE THERE IS BROADCASTING, THERE ARE 2 OPERATIONS : REPLICATION & MULTIPLICATION\n",
    "# counts_sum_inv.shape == torch.Size([32, 1])\n",
    "# dcounts = counts_sum_inv([32, 1]) * dprobs([32, 27])\n",
    "# broadcasting happens itself\n",
    "dcounts = counts_sum_inv * dprobs\n",
    "\n",
    "# check\n",
    "cmp(\"dcounts\", dcounts, counts)\n",
    "# NOTE \n",
    "# false because there is a second branch that depends on dcounts\n",
    "# counts_sum_inv = counts_sum**-1\n",
    "# counts_sum = counts.sum(1, keepdims=True)\n",
    "# counts_sum_inv also contributes to counts_sum which contributes to counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29a9ac5",
   "metadata": {},
   "source": [
    "### dloss / dcounts_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8b9e861a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dcounts_sum     | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# we have derivative of counts_sum_inv, next is counts_sum\n",
    "# counts_sum = counts.sum(1, keepdims=True)\n",
    "# counts_sum_inv = counts_sum**-1\n",
    "# dloss/dcounts_sum = dloss/dcounts_sum_inv * dcounts_sum_inv/dcounts_sum\n",
    "# dcounts_sum_inv/dcounts_sum ??\n",
    "# [ counts_sum_inv = counts_sum**-1 ] * d / d/dcounts_sum\n",
    "# dcounts_sum_inv/dcounts_sum = -counts_sum**-2\n",
    "# dloss/dcounts_sum = dloss/dcounts_sum_inv * -counts_sum**-2\n",
    "dcounts_sum = (-counts_sum**-2) * dcounts_sum_inv\n",
    "\n",
    "cmp(\"dcounts_sum\", dcounts_sum, counts_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fcea07",
   "metadata": {},
   "source": [
    "### dloss / dcounts part2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c6c0e114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dcounts         | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# counts.shape, counts_sum.shape\n",
    "# (torch.Size([32, 27]), torch.Size([32, 1]))\n",
    "\n",
    "# counts_sum = counts.sum(1, keepdims=True)\n",
    "\n",
    "# a11 a12 a13  --->  b1 (= a11 + a12 + a13)\n",
    "# a21 a22 a23  --->  b2 (= a21 + a22 + a23)\n",
    "# a31 a32 a33  --->  b3 (= a31 + a32 + a33)\n",
    "# \"how does b depends on a\" is what we are asking\n",
    "# b1 = a11 + a12 + a13\n",
    "# db1/a11 = 1\n",
    "# db1/a12 = 1\n",
    "# ..\n",
    "# so in chain rule\n",
    "# db1/a = local_derivative * derivative of b1\n",
    "# so basically for a1 1\n",
    "# dloss/a11 = local_derivative * derivative of b1 = db1/a11 * derivative of b1 = 1 * derivative of b1\n",
    "# hence it ADDITION IS LIKE A ROUTER\n",
    "# WHAT EVER GRADIENT WAS COMING FROM ABOVE IS NOW ROUTED/FLOWED EQUALLY FOR ALL ELEMENTS OF ADDITION\n",
    "# HENCE WE CAN JUST TAKE dcount_sum and replicate it row times\n",
    "\n",
    "# DOING += BECAUSE DCOUNTS IS BEING CALCULATED 2 TIMES SO WE ADD ITS GRADIENTS FROM 2 BRANCHES\n",
    "dcounts += torch.ones_like(counts) * dcounts_sum\n",
    "\n",
    "# check\n",
    "cmp(\"dcounts\", dcounts, counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232b0841",
   "metadata": {},
   "source": [
    "### dloss / dnorm_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "927926b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dnorm_logits    | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# NOW counts = norm_logits.exp()\n",
    "\n",
    "# dloss/dnorm_logits = local_derivative * derivative of counts\n",
    "# local_derivative = dcounts/dnorm_logits\n",
    "# local dericative of e^x is e^x\n",
    "\n",
    "dnorm_logits = counts * dcounts\n",
    "\n",
    "# check\n",
    "cmp(\"dnorm_logits\", dnorm_logits, norm_logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12d635b",
   "metadata": {},
   "source": [
    "### dloss / dlogits part1 & dloss / dlogit_maxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9d5d2924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 27]), torch.Size([32, 27]), torch.Size([32, 1]))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_logits.shape, logits.shape, logit_maxes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ed1ffdb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnorm_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "845b6486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dlogit_maxes    | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# NOW norm_logits = logits - logit_maxes\n",
    "\n",
    "# c11 c12 c13 = a11 a12 a13 - b1\n",
    "# c21 c22 c23 = a21 a22 a23 - b2\n",
    "# c31 c32 c33 = a31 a32 a33 - b3\n",
    "# so e.g. c32 = a32 - b3\n",
    "# local derivative of c for a is 1 and for b its -1\n",
    "\n",
    "dlogits = dnorm_logits.clone()  # not the final dlogits, function of dlogit_maxes too \n",
    "\n",
    "dlogit_maxes = (-dnorm_logits).sum(dim=1, keepdim=True)\n",
    "\n",
    "cmp(\"dlogit_maxes\", dlogit_maxes, logit_maxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d1049941",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.7253e-09],\n",
       "        [ 0.0000e+00],\n",
       "        [ 1.8626e-09],\n",
       "        [ 6.5193e-09],\n",
       "        [ 4.6566e-10],\n",
       "        [ 2.3283e-09],\n",
       "        [-2.3283e-09],\n",
       "        [ 2.7940e-09],\n",
       "        [-3.7253e-09],\n",
       "        [ 4.1910e-09],\n",
       "        [-1.8626e-09],\n",
       "        [-1.1176e-08],\n",
       "        [-2.3283e-10],\n",
       "        [-4.6566e-10],\n",
       "        [ 2.3283e-10],\n",
       "        [ 4.6566e-10],\n",
       "        [ 6.5193e-09],\n",
       "        [-4.6566e-09],\n",
       "        [ 2.7940e-09],\n",
       "        [-4.4238e-09],\n",
       "        [ 4.6566e-10],\n",
       "        [-1.8626e-09],\n",
       "        [ 1.6298e-09],\n",
       "        [ 1.3970e-09],\n",
       "        [ 1.8626e-09],\n",
       "        [ 1.3970e-09],\n",
       "        [ 4.6566e-10],\n",
       "        [-4.6566e-10],\n",
       "        [ 2.7940e-09],\n",
       "        [ 9.3132e-10],\n",
       "        [ 1.3970e-09],\n",
       "        [ 6.5193e-09]], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# norm_logits = logits - logit_maxes  # subtract max for numerical stability\n",
    "\n",
    "# since subtracting logit_maxes from logits doesnt affect probabilities\n",
    "# hence that means its gradient from here is 0\n",
    "# values of these do not matter wrt to final loss! \n",
    "# we can see this from dlogit_maxes (v small gradients)\n",
    "dlogit_maxes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f93561",
   "metadata": {},
   "source": [
    "### dloss / dlogit part2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f906bd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOW: logit_maxes = logits.max(1, keepdim=True).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cfb8207e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([[1.0336],\n",
       "        [0.8344],\n",
       "        [1.0872],\n",
       "        [0.6467],\n",
       "        [1.5847],\n",
       "        [0.8714],\n",
       "        [0.8120],\n",
       "        [1.3296],\n",
       "        [1.0376],\n",
       "        [1.0121],\n",
       "        [1.6896],\n",
       "        [1.9257],\n",
       "        [1.0903],\n",
       "        [0.8975],\n",
       "        [0.5977],\n",
       "        [0.8455],\n",
       "        [0.8936],\n",
       "        [0.7728],\n",
       "        [1.0903],\n",
       "        [0.8749],\n",
       "        [0.7572],\n",
       "        [1.0301],\n",
       "        [1.0903],\n",
       "        [1.1998],\n",
       "        [1.6018],\n",
       "        [1.1131],\n",
       "        [1.2348],\n",
       "        [1.0170],\n",
       "        [0.8961],\n",
       "        [0.7468],\n",
       "        [1.0592],\n",
       "        [0.8860]], grad_fn=<MaxBackward0>),\n",
       "indices=tensor([[ 1],\n",
       "        [ 2],\n",
       "        [19],\n",
       "        [ 4],\n",
       "        [15],\n",
       "        [25],\n",
       "        [16],\n",
       "        [ 3],\n",
       "        [ 7],\n",
       "        [ 8],\n",
       "        [15],\n",
       "        [ 3],\n",
       "        [22],\n",
       "        [18],\n",
       "        [ 7],\n",
       "        [ 5],\n",
       "        [ 2],\n",
       "        [ 1],\n",
       "        [22],\n",
       "        [19],\n",
       "        [15],\n",
       "        [19],\n",
       "        [22],\n",
       "        [22],\n",
       "        [23],\n",
       "        [ 5],\n",
       "        [22],\n",
       "        [20],\n",
       "        [24],\n",
       "        [ 8],\n",
       "        [24],\n",
       "        [ 4]]))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.max(1, keepdim=True)\n",
    "# returns indices of max logit too "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2caaf644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dlogits         | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# NOW: logit_maxes = logits.max(1, keepdim=True).values\n",
    "\n",
    "# gradients only flow back for logits that were max\n",
    "# 1 for max\n",
    "# hence dloss/dlogits = local_derivative * dlogit_maxes\n",
    "# local_derivative = 1 for where max indes was\n",
    "\n",
    "# ADDING BECAUSE THIS IS 2ND BRANCH FOR dlogits\n",
    "# populating 1 at max and 0 else where and then multilying by dlogit_maxes gradient\n",
    "dlogits += F.one_hot(logits.max(dim=1).indices, num_classes=logits.shape[1]) * dlogit_maxes\n",
    "\n",
    "# check\n",
    "cmp(\"dlogits\", dlogits, logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "914e763f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x12fd88ac0>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(F.one_hot(logits.max(dim=1).indices, num_classes=logits.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e96616f",
   "metadata": {},
   "source": [
    "### dloss/dh & dloss/W2 & dloss/b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "13d901ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 27]),\n",
       " torch.Size([32, 64]),\n",
       " torch.Size([64, 27]),\n",
       " torch.Size([27]))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlogits.shape, h.shape, W2.shape, b2.shape # broadcasting at b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "95a6b545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dh              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dW2             | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "b2              | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# NOW: logits = h @ W2 + b2  # output layer\n",
    "\n",
    "# Forward:\n",
    "# d = a @ b + c\n",
    "#\n",
    "# a: [2x2], b: [2x2], c: [1x2] (broadcasted)\n",
    "#\n",
    "# d11 = a11*b11 + a12*b21 + c1\n",
    "# d12 = a11*b12 + a12*b22 + c2\n",
    "# d21 = a21*b11 + a22*b21 + c1\n",
    "# d22 = a21*b12 + a22*b22 + c2\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# Gradient w.r.t. a  (already shown before, kept for flow)\n",
    "# =========================================================\n",
    "#\n",
    "# ∂L/∂a11 = ∂L/∂d11 * b11 + ∂L/∂d12 * b12\n",
    "# ∂L/∂a12 = ∂L/∂d11 * b21 + ∂L/∂d12 * b22\n",
    "#\n",
    "# ∂L/∂a21 = ∂L/∂d21 * b11 + ∂L/∂d22 * b12\n",
    "# ∂L/∂a22 = ∂L/∂d21 * b21 + ∂L/∂d22 * b22\n",
    "#\n",
    "# Matrix form:\n",
    "# ∂L/∂a = (∂L/∂d) @ bᵀ\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# Gradient w.r.t. b  (DERIVATION)\n",
    "# =========================================================\n",
    "#\n",
    "# Look at how each b_ij contributes to d:\n",
    "#\n",
    "# d11 = a11*b11 + a12*b21 + c1\n",
    "# d21 = a21*b11 + a22*b21 + c1\n",
    "#\n",
    "# So b11 affects d11 and d21\n",
    "#\n",
    "# ∂L/∂b11 = ∂L/∂d11 * a11 + ∂L/∂d21 * a21\n",
    "#\n",
    "#\n",
    "# d12 = a11*b12 + a12*b22 + c2\n",
    "# d22 = a21*b12 + a22*b22 + c2\n",
    "#\n",
    "# So b12 affects d12 and d22\n",
    "#\n",
    "# ∂L/∂b12 = ∂L/∂d12 * a11 + ∂L/∂d22 * a21\n",
    "#\n",
    "#\n",
    "# Similarly:\n",
    "#\n",
    "# ∂L/∂b21 = ∂L/∂d11 * a12 + ∂L/∂d21 * a22\n",
    "# ∂L/∂b22 = ∂L/∂d12 * a12 + ∂L/∂d22 * a22\n",
    "#\n",
    "#\n",
    "# Writing all together:\n",
    "#\n",
    "# [∂L/∂b11  ∂L/∂b12] = [a11 a21]ᵀ @ [∂L/∂d11  ∂L/∂d12]\n",
    "# [∂L/∂b21  ∂L/∂b22]   [a12 a22]    [∂L/∂d21  ∂L/∂d22]\n",
    "#\n",
    "# Matrix form:\n",
    "# ∂L/∂b = aᵀ @ (∂L/∂d)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# Gradient w.r.t. c  (DERIVATION)\n",
    "# =========================================================\n",
    "#\n",
    "# c is broadcasted across rows:\n",
    "#\n",
    "# d11 = ... + c1\n",
    "# d21 = ... + c1\n",
    "#\n",
    "# d12 = ... + c2\n",
    "# d22 = ... + c2\n",
    "#\n",
    "# So:\n",
    "#\n",
    "# ∂d11/∂c1 = 1,  ∂d21/∂c1 = 1\n",
    "# ∂d12/∂c2 = 1,  ∂d22/∂c2 = 1\n",
    "#\n",
    "# Applying chain rule:\n",
    "#\n",
    "# ∂L/∂c1 = ∂L/∂d11 * 1 + ∂L/∂d21 * 1\n",
    "# ∂L/∂c2 = ∂L/∂d12 * 1 + ∂L/∂d22 * 1\n",
    "#\n",
    "# Vector form:\n",
    "# ∂L/∂c = sum over rows of (∂L/∂d)\n",
    "#\n",
    "# i.e.\n",
    "# ∂L/∂c = (∂L/∂d).sum(dim=0)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# Final gradient identities (to memorize)\n",
    "# =========================================================\n",
    "#\n",
    "# If: d = a @ b + c\n",
    "#\n",
    "# then:\n",
    "# ∂L/∂a = (∂L/∂d) @ bᵀ\n",
    "# ∂L/∂b = aᵀ @ (∂L/∂d)\n",
    "# ∂L/∂c = (∂L/∂d).sum(0)\n",
    "#\n",
    "# This is EXACTLY what PyTorch autograd computes internally.\n",
    "\n",
    "# THE BACKWARD PASS OF A MATRIX MULTIPLY IS THE MATRIX MULIPLY\n",
    "\n",
    "# trick:\n",
    "# - dh should be shape of h : torch.Size([32, 64]\n",
    "# - logits = h @ W2 + b2\n",
    "# - we know dh should be some mulitplication of >>>dlogits with W2<<<\n",
    "# - dlogit is torch.Size([32, 27]\n",
    "# - W2 is torch.Size([64, 27]\n",
    "# - only way to multiply these matrices correctly is dlogits * W2^T\n",
    "dh = dlogits @ W2.T\n",
    "# similarly\n",
    "dW2 = h.T @ dlogits\n",
    "\n",
    "# bias is just the sum across rows\n",
    "# we know sum is across rows becuase we need db2 to to be te same size as b2: torch.Size([27]\n",
    "# dlogits shape is torch.Size([32, 27]\n",
    "# hence sum across dim=0\n",
    "db2 = dlogits.sum(dim=0)\n",
    "\n",
    "# check\n",
    "cmp(\"dh\", dh, h)\n",
    "cmp(\"dW2\", dW2, W2)\n",
    "cmp(\"b2\", db2, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "98952854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 27]),\n",
       " torch.Size([32, 64]),\n",
       " torch.Size([64, 27]),\n",
       " torch.Size([27]))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlogits.shape, h.shape, W2.shape, b2.shape  # broadcasting at b2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cba2e2d",
   "metadata": {},
   "source": [
    "### dloss / dhpreact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ae1f5d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dhpreact        | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# NOW: h = torch.tanh(hpreact)\n",
    "\n",
    "# dloss / dhpreact = local gradient * dh\n",
    "# local gradient = derivative of torch.tanh(hpreact)\n",
    "# d/dx(tanh(x)) = sech(x)^2 = 1 - tan(x)^2\n",
    "# derivative of tanh is 1-(output of tanh)^2\n",
    "\n",
    "dhpreact = (1.0 - h**2) * dh\n",
    "\n",
    "cmp(\"dhpreact\", dhpreact, hpreact)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62990076",
   "metadata": {},
   "source": [
    "### dloss/dbngain & dloss/dbnraw part1 & dloss/dbnbias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b6d7dd38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 64]),\n",
       " torch.Size([1, 64]),\n",
       " torch.Size([32, 64]),\n",
       " torch.Size([1, 64]))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hpreact.shape, bngain.shape, bnraw.shape, bnbias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "340c36ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 64])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dhpreact.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "cccdffff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dbngain         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dbnraw          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dbnbias         | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# NOW: hpreact = bngain * bnraw + bnbias\n",
    "\n",
    "# bngain size is torch.Size([1, 64]\n",
    "# elemnt-wise multiplication\n",
    "# bnraw * dhpreact will be [32, 64]\n",
    "# sum row wise\n",
    "dbngain = (bnraw * dhpreact).sum(dim=0, keepdim=True)\n",
    "dbnraw = bngain * dhpreact # broadcast \n",
    "# gradients just flow directly into biases always\n",
    "dbnbias = dhpreact.sum(dim=0, keepdim=True)\n",
    "\n",
    "cmp(\"dbngain\", dbngain, bngain)\n",
    "cmp(\"dbnraw\", dbnraw, bnraw)\n",
    "cmp(\"dbnbias\", dbnbias, bnbias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c596d28",
   "metadata": {},
   "source": [
    "### dbndiff part1 & bndiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "355caf6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 64]), torch.Size([32, 64]), torch.Size([1, 64]))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnraw.shape, bndiff.shape, bnvar_inv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2b6c4a9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 64])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbnraw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ee716cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dbndiff         | exact: False | approximate: False | maxdiff: 0.0011457442305982113\n",
      "dbnvar_inv      | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# NOW: bnraw = bndiff * bnvar_inv\n",
    "\n",
    "# part1: has another branch\n",
    "dbndiff = bnvar_inv * dbnraw\n",
    "# keep checking dims of result and the element wise multiplication elements\n",
    "# to see if the result x matches dx\n",
    "dbnvar_inv = (bndiff * dbnraw).sum(dim=0, keepdim=True)\n",
    "\n",
    "cmp(\"dbndiff\", dbndiff, bndiff)\n",
    "cmp(\"dbnvar_inv\", dbnvar_inv, bnvar_inv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8f2d38",
   "metadata": {},
   "source": [
    "### dbnvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0a1635d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dbnvar          | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# NOW: bnvar_inv = (bnvar + 1e-5) ** -0.5\n",
    "\n",
    "dbnvar = (-0.5 * (bnvar + 1e-5) ** -1.5) * 1 * dbnvar_inv\n",
    "\n",
    "cmp(\"dbnvar\", dbnvar, bnvar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad0ea77",
   "metadata": {},
   "source": [
    "### dbndiff2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "785a59e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 64]), torch.Size([32, 64]))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnvar.shape, bndiff2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a19ae434",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbnvar.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "edd997b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dbndiff2        | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# NOW : bnvar = ( 1 / (n - 1) * (bndiff2).sum(0, keepdim=True))  # note: Bessel's correction (dividing by n-1, not n)\n",
    "\n",
    "# WHEN WE HAVE A SUM IN FORWARD PASS\n",
    "# IT TURNS INTO BROADCAST IN BACKWARD\n",
    "\n",
    "# WHEN WE HAVE A BROADCAST IN FORWARD\n",
    "# IT TURNS INTO SUM IN BACKWARD\n",
    "\n",
    "# a11  a12\n",
    "# a21  a22\n",
    "# ---->\n",
    "# b1, b2, where:\n",
    "# b1 = 1/(n-1) * (a11 + a21)\n",
    "# b2 = 1/(n-1) * (a12 + a22)\n",
    "\n",
    "# derivative of b1 wrt a11, a21\n",
    "# is 1/(n-1) * db1\n",
    "# replicating across all row sum\n",
    "\n",
    "dbndiff2 = (1 / (n - 1)) * torch.ones_like(bndiff2) * dbnvar # broadcasting \n",
    "\n",
    "cmp(\"dbndiff2\", dbndiff2, bndiff2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e7af87",
   "metadata": {},
   "source": [
    "### dbndiff part2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "00c46165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dbndiff         | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# NOW: bndiff2 = bndiff**2\n",
    "\n",
    "dbndiff += (2 * bndiff) * dbndiff2\n",
    "\n",
    "cmp(\"dbndiff\", dbndiff, bndiff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef21d3b",
   "metadata": {},
   "source": [
    "### dhprebn part1 & dbnmeani"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "afd8043b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 64]), torch.Size([32, 64]), torch.Size([1, 64]))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bndiff.shape, hprebn.shape, bnmeani.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "477b24d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dhprebn         | exact: False | approximate: False | maxdiff: 0.0010110712610185146\n",
      "dbnmeani        | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# NOW: bndiff = hprebn - bnmeani\n",
    "\n",
    "dhprebn = 1.0 * dbndiff.clone()\n",
    "dbnmeani = (-dbndiff).sum(dim=0, keepdim=True)\n",
    "\n",
    "# is wrong because 2 branch\n",
    "cmp(\"dhprebn\", dhprebn, hprebn)\n",
    "cmp(\"dbnmeani\", dbnmeani, bnmeani)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f910442b",
   "metadata": {},
   "source": [
    "### dhprebn part2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "867873c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dhprebn         | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# NOW: bnmeani = 1 / n * hprebn.sum(0, keepdim=True)\n",
    "\n",
    "dhprebn +=  1.0 / n * (torch.ones_like(hprebn) * dbnmeani)\n",
    "\n",
    "cmp(\"dhprebn\", dhprebn, hprebn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6828e2ac",
   "metadata": {},
   "source": [
    "### dembcat & dW1 & db1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8a7e6ab3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 64]),\n",
       " torch.Size([32, 30]),\n",
       " torch.Size([30, 64]),\n",
       " torch.Size([64]))"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hprebn.shape, embcat.shape, W1.shape, b1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b28ae791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dembcat         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dW1             | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "db1             | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# NOW: hprebn = embcat @ W1 + b1  # hidden layer pre-activation\n",
    "\n",
    "# do the trick here !\n",
    "# dembcat [32, 30]\n",
    "dembcat = dhprebn @ W1.T \n",
    "\n",
    "# dW1 [30, 64]\n",
    "dW1 = embcat.T @ dhprebn\n",
    "\n",
    "# db1 [64]\n",
    "db1 = dhprebn.sum(dim=0)\n",
    "\n",
    "cmp(\"dembcat\", dembcat, embcat)\n",
    "cmp(\"dW1\", dW1, W1)\n",
    "cmp(\"db1\", db1, b1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d169f82",
   "metadata": {},
   "source": [
    "### demb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "22a6ab7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 30]), torch.Size([32, 3, 10]))"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embcat.shape, emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e879a260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demb            | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# NOW: embcat = emb.view(emb.shape[0], -1)  # concatenate the vectors\n",
    "# concating torch.Size([32, 3, 10] 3, 10 dimensional\n",
    "# so just need to unconcat it\n",
    "demb = dembcat.view(emb.shape)\n",
    "\n",
    "cmp(\"demb\", demb, emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df977d11",
   "metadata": {},
   "source": [
    "### dC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0989818f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3, 10]), torch.Size([27, 10]))"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.shape , C.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f5414e5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "973077f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dC              | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# NOW: emb = C[Xb]  # embed the characters into vectors\n",
    "# C row represents rows --> embeddings for a char\n",
    "# if a row was used multiple times we need to add them too\n",
    "\n",
    "# we took row of C as index\n",
    "# and added it to emb at k,j for embeddings\n",
    "dC = torch.zeros_like(C)\n",
    "for k in range(Xb.shape[0]):\n",
    "    for j in range(Xb.shape[1]):\n",
    "        ix = Xb[k,j]\n",
    "        dC[ix] += demb[k,j]\n",
    "\n",
    "cmp(\"dC\", dC, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27de91a5",
   "metadata": {},
   "source": [
    "## bringing it together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba37cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: backprop through the whole thing manually,\n",
    "# backpropagating through exactly all of the variables\n",
    "# as they are defined in the forward pass above, one by one\n",
    "\n",
    "# dlogprobs = torch.zeros_like(logprobs)\n",
    "# dlogprobs[range(n), Yb] = -1.0 / n\n",
    "# dprobs = (1.0 / probs) * dlogprobs\n",
    "# dcounts_sum_inv = (counts * dprobs).sum(1, keepdim=True)\n",
    "# dcounts = counts_sum_inv * dprobs\n",
    "# dcounts_sum = (-(counts_sum**-2)) * dcounts_sum_inv\n",
    "# dcounts += torch.ones_like(counts) * dcounts_sum\n",
    "# dnorm_logits = counts * dcounts\n",
    "# dlogits = dnorm_logits.clone()\n",
    "# dlogit_maxes = (-dnorm_logits).sum(1, keepdim=True)\n",
    "# dlogits += F.one_hot(logits.max(1).indices, num_classes=logits.shape[1]) * dlogit_maxes\n",
    "# dh = dlogits @ W2.T\n",
    "# dW2 = h.T @ dlogits\n",
    "# db2 = dlogits.sum(0)\n",
    "# dhpreact = (1.0 - h**2) * dh\n",
    "# dbngain = (bnraw * dhpreact).sum(0, keepdim=True)\n",
    "# dbnraw = bngain * dhpreact\n",
    "# dbnbias = dhpreact.sum(0, keepdim=True)\n",
    "# dbndiff = bnvar_inv * dbnraw\n",
    "# dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n",
    "# dbnvar = (-0.5 * (bnvar + 1e-5) ** -1.5) * dbnvar_inv\n",
    "# dbndiff2 = (1.0 / (n - 1)) * torch.ones_like(bndiff2) * dbnvar\n",
    "# dbndiff += (2 * bndiff) * dbndiff2\n",
    "# dhprebn = dbndiff.clone()\n",
    "# dbnmeani = (-dbndiff).sum(0)\n",
    "# dhprebn += 1.0 / n * (torch.ones_like(hprebn) * dbnmeani)\n",
    "# dembcat = dhprebn @ W1.T\n",
    "# dW1 = embcat.T @ dhprebn\n",
    "# db1 = dhprebn.sum(0)\n",
    "# demb = dembcat.view(emb.shape)\n",
    "# dC = torch.zeros_like(C)\n",
    "# for k in range(Xb.shape[0]):\n",
    "#     for j in range(Xb.shape[1]):\n",
    "#         ix = Xb[k, j]\n",
    "#         dC[ix] += demb[k, j]\n",
    "\n",
    "# cmp(\"logprobs\", dlogprobs, logprobs)\n",
    "# cmp(\"probs\", dprobs, probs)\n",
    "# cmp(\"counts_sum_inv\", dcounts_sum_inv, counts_sum_inv)\n",
    "# cmp(\"counts_sum\", dcounts_sum, counts_sum)\n",
    "# cmp(\"counts\", dcounts, counts)\n",
    "# cmp(\"norm_logits\", dnorm_logits, norm_logits)\n",
    "# cmp(\"logit_maxes\", dlogit_maxes, logit_maxes)\n",
    "# cmp(\"logits\", dlogits, logits)\n",
    "# cmp(\"h\", dh, h)\n",
    "# cmp(\"W2\", dW2, W2)\n",
    "# cmp(\"b2\", db2, b2)\n",
    "# cmp(\"hpreact\", dhpreact, hpreact)\n",
    "# cmp(\"bngain\", dbngain, bngain)\n",
    "# cmp(\"bnbias\", dbnbias, bnbias)\n",
    "# cmp(\"bnraw\", dbnraw, bnraw)\n",
    "# cmp(\"bnvar_inv\", dbnvar_inv, bnvar_inv)\n",
    "# cmp(\"bnvar\", dbnvar, bnvar)\n",
    "# cmp(\"bndiff2\", dbndiff2, bndiff2)\n",
    "# cmp(\"bndiff\", dbndiff, bndiff)\n",
    "# cmp(\"bnmeani\", dbnmeani, bnmeani)\n",
    "# cmp(\"hprebn\", dhprebn, hprebn)\n",
    "# cmp(\"embcat\", dembcat, embcat)\n",
    "# cmp(\"W1\", dW1, W1)\n",
    "# cmp(\"b1\", db1, b1)\n",
    "# cmp(\"emb\", demb, emb)\n",
    "# cmp(\"C\", dC, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9becd625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3539576530456543 diff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Exercise 2: backprop through cross_entropy but all in one go\n",
    "# to complete this challenge look at the mathematical expression of the loss,\n",
    "# take the derivative, simplify the expression, and just write it out\n",
    "\n",
    "# forward pass\n",
    "\n",
    "# before:\n",
    "# logit_maxes = logits.max(1, keepdim=True).values\n",
    "# norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
    "# counts = norm_logits.exp()\n",
    "# counts_sum = counts.sum(1, keepdims=True)\n",
    "# counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
    "# probs = counts * counts_sum_inv\n",
    "# logprobs = probs.log()\n",
    "# loss = -logprobs[range(n), Yb].mean()\n",
    "\n",
    "# now:\n",
    "loss_fast = F.cross_entropy(logits, Yb)\n",
    "print(loss_fast.item(), \"diff:\", (loss_fast - loss).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275a2c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Softmax + Cross-Entropy (single example x, label y)\n",
    "# Full forward + backward derivation in comments\n",
    "# ============================================================\n",
    "\n",
    "# Forward pass:\n",
    "#\n",
    "# x --(NN with weights W)--> logits (l)\n",
    "# logits --softmax--> probs (p)\n",
    "#\n",
    "# probs[y] --(-log)--> loss\n",
    "#\n",
    "# -----------------------------------------\n",
    "# Softmax:\n",
    "#\n",
    "# p_i = exp(l_i) / sum_j exp(l_j)\n",
    "#\n",
    "# -----------------------------------------\n",
    "# Loss (negative log-likelihood):\n",
    "#\n",
    "# loss = -log(p_y)\n",
    "#      = -log( exp(l_y) / sum_j exp(l_j) )\n",
    "#\n",
    "# -----------------------------------------\n",
    "# Backward pass: d(loss) / d(l_i)\n",
    "#\n",
    "# Start from:\n",
    "#\n",
    "# loss = -log( exp(l_y) / sum_j exp(l_j) )\n",
    "#\n",
    "# Using:\n",
    "# d/dx [log x] = 1 / x\n",
    "#\n",
    "# -----------------------------------------\n",
    "# Case 1: i != y\n",
    "#\n",
    "# d(loss)/d(l_i)\n",
    "# = - d/d(l_i) [ log( exp(l_y) / sum_j exp(l_j) ) ]\n",
    "#\n",
    "# = - (sum_j exp(l_j) / exp(l_y)) * d/d(l_i) [ exp(l_y) / sum_j exp(l_j) ]\n",
    "#\n",
    "# Since i != y:\n",
    "# d(exp(l_y))/d(l_i) = 0\n",
    "#\n",
    "# Only denominator contributes:\n",
    "#\n",
    "# = + exp(l_i) / sum_j exp(l_j)\n",
    "#\n",
    "# = p_i\n",
    "#\n",
    "# -----------------------------------------\n",
    "# Case 2: i == y\n",
    "#\n",
    "# d(loss)/d(l_y)\n",
    "# = - d/d(l_y) [ log( exp(l_y) / sum_j exp(l_j) ) ]\n",
    "#\n",
    "# Apply quotient + product rule:\n",
    "#\n",
    "# = - [ (sum_j exp(l_j) * exp(l_y) - exp(l_y) * exp(l_y)) / (sum_j exp(l_j))^2 ]\n",
    "#\n",
    "# = - [ (sum_j exp(l_j) - exp(l_y)) / sum_j exp(l_j) ]\n",
    "#\n",
    "# = exp(l_y) / sum_j exp(l_j) - 1\n",
    "#\n",
    "# = p_y - 1\n",
    "#\n",
    "# -----------------------------------------\n",
    "# Final gradient (combined form):\n",
    "#\n",
    "# d(loss)/d(l_i) =\n",
    "#   p_i - 1    if i == y\n",
    "#   p_i        if i != y\n",
    "#\n",
    "# -----------------------------------------\n",
    "# Vector form (very important):\n",
    "#\n",
    "# dL/dlogits = probs\n",
    "# dL/dlogits[y] -= 1\n",
    "#\n",
    "# -----------------------------------------\n",
    "# Key insight:\n",
    "#\n",
    "# Softmax + Cross-Entropy collapses into a\n",
    "# single clean gradient step:\n",
    "#\n",
    "#     grad = probs - one_hot(y)\n",
    "#\n",
    "# This is why frameworks (PyTorch, TensorFlow)\n",
    "# fuse softmax + cross-entropy into ONE operation\n",
    "# for numerical stability and efficiency.\n",
    "#\n",
    "# ============================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d37da2",
   "metadata": {},
   "source": [
    "# Cross-Entropy Loss and Backward Pass (Mathematical Derivation)\n",
    "\n",
    "We start from one training example.\n",
    "\n",
    "Given\n",
    "\n",
    "logits:\n",
    "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi mathvariant=\"bold\">z</mi><mo>=</mo><mo stretchy=\"false\">(</mo><msub><mi>z</mi><mn>1</mn></msub><mo separator=\"true\">,</mo><msub><mi>z</mi><mn>2</mn></msub><mo separator=\"true\">,</mo><mo>…</mo><mo separator=\"true\">,</mo><msub><mi>z</mi><mi>K</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\mathbf{z} = (z_1, z_2, \\dots, z_K)</annotation></semantics></math>\n",
    "true class index:\n",
    "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>y</mi></mrow><annotation encoding=\"application/x-tex\">y</annotation></semantics></math>\n",
    "\n",
    "Step 1: Softmax\n",
    "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><msub><mi>p</mi><mi>i</mi></msub><mo>=</mo><mfrac><msup><mi>e</mi><msub><mi>z</mi><mi>i</mi></msub></msup><mrow><munder><mo>∑</mo><mi>j</mi></munder><msup><mi>e</mi><msub><mi>z</mi><mi>j</mi></msub></msup></mrow></mfrac></mrow><annotation encoding=\"application/x-tex\">p_i = \\frac{e^{z_i}}{\\sum_j e^{z_j}}</annotation></semantics></math>\n",
    "Step 2: Cross-entropy loss\n",
    "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>L</mi><mo>=</mo><mo>−</mo><munder><mo>∑</mo><mi>i</mi></munder><msub><mi>y</mi><mi>i</mi></msub><mi>log</mi><mo>⁡</mo><msub><mi>p</mi><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">L = -\\sum_i y_i \\log p_i</annotation></semantics></math>\n",
    "where y_i is one-hot.\n",
    "\n",
    "Since only y_y = 1, everything else is zero:\n",
    "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><menclose notation=\"box\"><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mstyle scriptlevel=\"0\" displaystyle=\"true\"><mrow><mi>L</mi><mo>=</mo><mo>−</mo><mi>log</mi><mo>⁡</mo><mo stretchy=\"false\">(</mo><msub><mi>p</mi><mi>y</mi></msub><mo stretchy=\"false\">)</mo></mrow></mstyle></mstyle></mstyle></menclose></mrow><annotation encoding=\"application/x-tex\">\\boxed{L = -\\log(p_y)}</annotation></semantics></math>\n",
    "\n",
    "## Rewrite loss in a differentiable form\n",
    "Now plug softmax into the loss.\n",
    "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>L</mi><mo>=</mo><mo>−</mo><mi>log</mi><mo>⁡</mo><mrow><mo fence=\"true\">(</mo><mfrac><msup><mi>e</mi><msub><mi>z</mi><mi>y</mi></msub></msup><mrow><munder><mo>∑</mo><mi>j</mi></munder><msup><mi>e</mi><msub><mi>z</mi><mi>j</mi></msub></msup></mrow></mfrac><mo fence=\"true\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">L = -\\log\\left(\\frac{e^{z_y}}{\\sum_j e^{z_j}}\\right)</annotation></semantics></math>\n",
    "Use log rules:\n",
    "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><menclose notation=\"box\"><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mstyle scriptlevel=\"0\" displaystyle=\"true\"><mrow><mi>L</mi><mo>=</mo><mo>−</mo><msub><mi>z</mi><mi>y</mi></msub><mo>+</mo><mi>log</mi><mo>⁡</mo><mrow><mo fence=\"true\">(</mo><munder><mo>∑</mo><mi>j</mi></munder><msup><mi>e</mi><msub><mi>z</mi><mi>j</mi></msub></msup><mo fence=\"true\">)</mo></mrow></mrow></mstyle></mstyle></mstyle></menclose></mrow><annotation encoding=\"application/x-tex\">\\boxed{L = -z_y + \\log\\left(\\sum_j e^{z_j}\\right)}</annotation></semantics></math>\n",
    "This step is crucial because now the loss is written directly in terms of logits.\n",
    "\n",
    "## Differentiate w.r.t. logits\n",
    "We now compute:\n",
    "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mfrac><mrow><mi mathvariant=\"normal\">∂</mi><mi>L</mi></mrow><mrow><mi mathvariant=\"normal\">∂</mi><msub><mi>z</mi><mi>i</mi></msub></mrow></mfrac></mrow><annotation encoding=\"application/x-tex\">\\frac{\\partial L}{\\partial z_i}</annotation></semantics></math>\n",
    "Take derivatives term by term.\n",
    "\n",
    "### Term 1: -z_y\n",
    "\n",
    "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mfrac><mrow><mi mathvariant=\"normal\">∂</mi><mo stretchy=\"false\">(</mo><mo>−</mo><msub><mi>z</mi><mi>y</mi></msub><mo stretchy=\"false\">)</mo></mrow><mrow><mi mathvariant=\"normal\">∂</mi><msub><mi>z</mi><mi>i</mi></msub></mrow></mfrac><mo>=</mo><mrow><mo fence=\"true\">{</mo><mtable rowspacing=\"0.36em\" columnalign=\"left left\" columnspacing=\"1em\"><mtr><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mrow><mo>−</mo><mn>1</mn></mrow></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mrow><mi>i</mi><mo>=</mo><mi>y</mi></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mrow><mi>i</mi><mo mathvariant=\"normal\">≠</mo><mi>y</mi></mrow></mstyle></mtd></mtr></mtable></mrow></mrow><annotation encoding=\"application/x-tex\">\\frac{\\partial (-z_y)}{\\partial z_i}=\\begin{cases}-1 &amp; i = y \\\\0 &amp; i \\neq y\\end{cases}</annotation></semantics></math>\n",
    "\n",
    "Term 2: log(sigma_j(e^zj))\n",
    "\n",
    "Apply chain rule:\n",
    "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mfrac><mi mathvariant=\"normal\">∂</mi><mrow><mi mathvariant=\"normal\">∂</mi><msub><mi>z</mi><mi>i</mi></msub></mrow></mfrac><mi>log</mi><mo>⁡</mo><mrow><mo fence=\"true\">(</mo><munder><mo>∑</mo><mi>j</mi></munder><msup><mi>e</mi><msub><mi>z</mi><mi>j</mi></msub></msup><mo fence=\"true\">)</mo></mrow><mo>=</mo><mfrac><mn>1</mn><mrow><munder><mo>∑</mo><mi>j</mi></munder><msup><mi>e</mi><msub><mi>z</mi><mi>j</mi></msub></msup></mrow></mfrac><mo>⋅</mo><msup><mi>e</mi><msub><mi>z</mi><mi>i</mi></msub></msup></mrow><annotation encoding=\"application/x-tex\">\\frac{\\partial}{\\partial z_i}\n",
    "\\log\\left(\\sum_j e^{z_j}\\right)=\\frac{1}{\\sum_j e^{z_j}} \\cdot e^{z_i}</annotation></semantics></math>\n",
    "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><menclose notation=\"box\"><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mstyle scriptlevel=\"0\" displaystyle=\"true\"><mrow><mo>=</mo><mfrac><msup><mi>e</mi><msub><mi>z</mi><mi>i</mi></msub></msup><mrow><munder><mo>∑</mo><mi>j</mi></munder><msup><mi>e</mi><msub><mi>z</mi><mi>j</mi></msub></msup></mrow></mfrac><mo>=</mo><msub><mi>p</mi><mi>i</mi></msub></mrow></mstyle></mstyle></mstyle></menclose></mrow><annotation encoding=\"application/x-tex\">\\boxed{= \\frac{e^{z_i}}{\\sum_j e^{z_j}} = p_i}</annotation></semantics></math>\n",
    "This is exactly the softmax probability.\n",
    "\n",
    "### Combine both terms\n",
    "\n",
    "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mfrac><mrow><mi mathvariant=\"normal\">∂</mi><mi>L</mi></mrow><mrow><mi mathvariant=\"normal\">∂</mi><msub><mi>z</mi><mi>i</mi></msub></mrow></mfrac><mo>=</mo><msub><mi>p</mi><mi>i</mi></msub><mo>−</mo><msub><mn mathvariant=\"bold\">1</mn><mrow><mi>i</mi><mo>=</mo><mi>y</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">\\frac{\\partial L}{\\partial z_i}=p_i - \\mathbf{1}_{i=y}</annotation></semantics></math>\n",
    "\n",
    "## Why this becomes (softmax − one_hot)\n",
    "Left:\n",
    "\n",
    "probs = softmax(logits)\n",
    "\n",
    "y = true class index\n",
    "\n",
    "Then mathematically:\n",
    "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><msub><mi mathvariant=\"normal\">∇</mi><mtext>logits</mtext></msub><mi>L</mi><mo>=</mo><mtext>probs</mtext><mo>−</mo><mtext>one_hot</mtext><mo stretchy=\"false\">(</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\nabla_{\\text{logits}} L = \\text{probs} - \\text{one\\_hot}(y)</annotation></semantics></math>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0767ec82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dlogits         | exact: False | approximate: True  | maxdiff: 7.450580596923828e-09\n"
     ]
    }
   ],
   "source": [
    "# backward pass\n",
    "dlogits = F.softmax(logits, dim=1)\n",
    "dlogits[range(n), Yb] -= 1 \n",
    "dlogits /= n \n",
    "\n",
    "cmp(\"dlogits\", dlogits, logits) # approximate bec of floating points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a85ec5fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0614, 0.0907, 0.0188, 0.0474, 0.0213, 0.0803, 0.0232, 0.0366, 0.0172,\n",
       "        0.0296, 0.0408, 0.0354, 0.0361, 0.0304, 0.0385, 0.0141, 0.0091, 0.0194,\n",
       "        0.0177, 0.0534, 0.0505, 0.0212, 0.0263, 0.0765, 0.0567, 0.0255, 0.0220],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(logits, 1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "0dbf9a14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0614,  0.0907,  0.0188,  0.0474,  0.0213,  0.0803,  0.0232,  0.0366,\n",
       "        -0.9828,  0.0296,  0.0408,  0.0354,  0.0361,  0.0304,  0.0385,  0.0141,\n",
       "         0.0091,  0.0194,  0.0177,  0.0534,  0.0505,  0.0212,  0.0263,  0.0765,\n",
       "         0.0567,  0.0255,  0.0220], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlogits[0] * n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a3db54",
   "metadata": {},
   "source": [
    "i ≠ y (wrong classes)\n",
    "- “Push this logit DOWN proportional to how confident you were”\n",
    "\n",
    "i = y (correct class)\n",
    "- Zero gradient when p_y = 1\n",
    "- Negative gradient when p_y < 1 (push it up)\n",
    "\n",
    "For all classes at once:\n",
    "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><menclose notation=\"box\"><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mstyle scriptlevel=\"0\" displaystyle=\"true\"><mrow><mfrac><mrow><mi mathvariant=\"normal\">∂</mi><mi>L</mi></mrow><mrow><mi mathvariant=\"normal\">∂</mi><mtext>logits</mtext></mrow></mfrac><mo>=</mo><mtext>probs</mtext><mo>−</mo><mtext>one_hot</mtext><mo stretchy=\"false\">(</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow></mstyle></mstyle></mstyle></menclose></mrow><annotation encoding=\"application/x-tex\">\\boxed{\n",
    "\\frac{\\partial L}{\\partial \\text{logits}} = \\text{probs} - \\text{one\\_hot}(y)\n",
    "}</annotation></semantics></math>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ef7e5412",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.8626e-09, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlogits[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0d91206e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1377d8400>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(4, 4))\n",
    "plt.imshow(dlogits.detach(), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "36f4fe98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max diff: tensor(4.7684e-07, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3: backprop through batchnorm but all in one go\n",
    "# to complete this challenge look at the mathematical expression of the output of batchnorm,\n",
    "# take the derivative w.r.t. its input, simplify the expression, and just write it out\n",
    "\n",
    "# forward pass\n",
    "\n",
    "# before:\n",
    "# bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
    "# bndiff = hprebn - bnmeani\n",
    "# bndiff2 = bndiff**2\n",
    "# bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
    "# bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "# bnraw = bndiff * bnvar_inv\n",
    "# hpreact = bngain * bnraw + bnbias\n",
    "\n",
    "# now:\n",
    "hpreact_fast = (\n",
    "    bngain\n",
    "    * (hprebn - hprebn.mean(0, keepdim=True))\n",
    "    / torch.sqrt(hprebn.var(0, keepdim=True, unbiased=True) + 1e-5)\n",
    "    + bnbias\n",
    ")\n",
    "print(\"max diff:\", (hpreact_fast - hpreact).abs().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169f85b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# backward pass\n",
    "\n",
    "# before we had:\n",
    "# dbnraw = bngain * dhpreact\n",
    "# dbndiff = bnvar_inv * dbnraw\n",
    "# dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n",
    "# dbnvar = (-0.5*(bnvar + 1e-5)**-1.5) * dbnvar_inv\n",
    "# dbndiff2 = (1.0/(n-1))*torch.ones_like(bndiff2) * dbnvar\n",
    "# dbndiff += (2*bndiff) * dbndiff2\n",
    "# dhprebn = dbndiff.clone()\n",
    "# dbnmeani = (-dbndiff).sum(0)\n",
    "# dhprebn += 1.0/n * (torch.ones_like(hprebn) * dbnmeani)\n",
    "\n",
    "# calculate dhprebn given dhpreact (i.e. backprop through the batchnorm)\n",
    "# (you'll also need to use some of the variables from the forward pass up above)\n",
    "\n",
    "dhprebn = (\n",
    "    bngain\n",
    "    * bnvar_inv\n",
    "    / n\n",
    "    * (n * dhpreact - dhpreact.sum(0) - n / (n - 1) * bnraw * (dhpreact * bnraw).sum(0))\n",
    ")\n",
    "\n",
    "cmp(\n",
    "    \"hprebn\", dhprebn, hprebn\n",
    ")  # I can only get approximate to be true, my maxdiff is 9e-10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e57f2e",
   "metadata": {},
   "source": [
    "Each <math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>x</mi><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">x_i</annotation></semantics></math> affects the loss in three different ways:\n",
    "\n",
    "- Directly through its own normalized value <math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mover accent=\"true\"><mi>x</mi><mo>^</mo></mover><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\hat{x}_i</annotation></semantics></math>\n",
    "- Indirectly through the batch mean <math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>μ</mi></mrow><annotation encoding=\"application/x-tex\">\\mu</annotation></semantics></math>\n",
    "- Indirectly through the batch variance <math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msup><mi>σ</mi><mn>2</mn></msup></mrow><annotation encoding=\"application/x-tex\">\\sigma^2</annotation></semantics></math>\n",
    "\n",
    "So by the chain rule: \n",
    "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><menclose notation=\"box\"><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mstyle scriptlevel=\"0\" displaystyle=\"true\"><mrow><mfrac><mrow><mi mathvariant=\"normal\">∂</mi><mi>L</mi></mrow><mrow><mi mathvariant=\"normal\">∂</mi><msub><mi>x</mi><mi>i</mi></msub></mrow></mfrac><mo>=</mo><mfrac><mrow><mi mathvariant=\"normal\">∂</mi><mi>L</mi></mrow><mrow><mi mathvariant=\"normal\">∂</mi><msub><mover accent=\"true\"><mi>x</mi><mo>^</mo></mover><mi>i</mi></msub></mrow></mfrac><mfrac><mrow><mi mathvariant=\"normal\">∂</mi><msub><mover accent=\"true\"><mi>x</mi><mo>^</mo></mover><mi>i</mi></msub></mrow><mrow><mi mathvariant=\"normal\">∂</mi><msub><mi>x</mi><mi>i</mi></msub></mrow></mfrac><mo>+</mo><mfrac><mrow><mi mathvariant=\"normal\">∂</mi><mi>L</mi></mrow><mrow><mi mathvariant=\"normal\">∂</mi><mi>μ</mi></mrow></mfrac><mfrac><mrow><mi mathvariant=\"normal\">∂</mi><mi>μ</mi></mrow><mrow><mi mathvariant=\"normal\">∂</mi><msub><mi>x</mi><mi>i</mi></msub></mrow></mfrac><mo>+</mo><mfrac><mrow><mi mathvariant=\"normal\">∂</mi><mi>L</mi></mrow><mrow><mi mathvariant=\"normal\">∂</mi><msup><mi>σ</mi><mn>2</mn></msup></mrow></mfrac><mfrac><mrow><mi mathvariant=\"normal\">∂</mi><msup><mi>σ</mi><mn>2</mn></msup></mrow><mrow><mi mathvariant=\"normal\">∂</mi><msub><mi>x</mi><mi>i</mi></msub></mrow></mfrac></mrow></mstyle></mstyle></mstyle></menclose></mrow><annotation encoding=\"application/x-tex\">\\boxed{\n",
    "\\frac{\\partial L}{\\partial x_i}=\\frac{\\partial L}{\\partial \\hat{x}_i}\\frac{\\partial \\hat{x}_i{\\partial x_i}+\\frac{\\partial L}{\\partial \\mu}\\frac{\\partial \\mu}{\\partial x_i}+\\frac{\\partial L}{\\partial \\sigma^2}\\frac{\\partial \\sigma^2}{\\partial x_i}}</annotation></semantics></math>\n",
    "\n",
    "### Gradient flows through γ first\n",
    "From:\n",
    "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><msub><mi>y</mi><mi>i</mi></msub><mo>=</mo><mi>γ</mi><msub><mover accent=\"true\"><mi>x</mi><mo>^</mo></mover><mi>i</mi></msub><mo>+</mo><mi>β</mi></mrow><annotation encoding=\"application/x-tex\">y_i = \\gamma \\hat{x}_i + \\beta</annotation></semantics></math>\n",
    "Derivative:\n",
    "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mfrac><mrow><mi mathvariant=\"normal\">∂</mi><mi>L</mi></mrow><mrow><mi mathvariant=\"normal\">∂</mi><msub><mover accent=\"true\"><mi>x</mi><mo>^</mo></mover><mi>i</mi></msub></mrow></mfrac><mo>=</mo><mfrac><mrow><mi mathvariant=\"normal\">∂</mi><mi>L</mi></mrow><mrow><mi mathvariant=\"normal\">∂</mi><msub><mi>y</mi><mi>i</mi></msub></mrow></mfrac><mo>⋅</mo><mi>γ</mi></mrow><annotation encoding=\"application/x-tex\">\\frac{\\partial L}{\\partial \\hat{x}_i}=\\frac{\\partial L}{\\partial y_i} \\cdot \\gamma</annotation></semantics></math>\n",
    "\n",
    "### Gradient wrt variance \n",
    "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><msub><mover accent=\"true\"><mi>x</mi><mo>^</mo></mover><mi>i</mi></msub><mo>=</mo><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>−</mo><mi>μ</mi><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">(</mo><msup><mi>σ</mi><mn>2</mn></msup><mo>+</mo><mi>ε</mi><msup><mo stretchy=\"false\">)</mo><mrow><mo>−</mo><mn>1</mn><mi mathvariant=\"normal\">/</mi><mn>2</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\hat{x}_i = (x_i - \\mu)(\\sigma^2 + \\varepsilon)^{-1/2}</annotation></semantics></math>\n",
    "Differentiate wrt variance\n",
    "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mfrac><mrow><mi mathvariant=\"normal\">∂</mi><msub><mover accent=\"true\"><mi>x</mi><mo>^</mo></mover><mi>i</mi></msub></mrow><mrow><mi mathvariant=\"normal\">∂</mi><msup><mi>σ</mi><mn>2</mn></msup></mrow></mfrac><mo>=</mo><mo>−</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>−</mo><mi>μ</mi><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">(</mo><msup><mi>σ</mi><mn>2</mn></msup><mo>+</mo><mi>ε</mi><msup><mo stretchy=\"false\">)</mo><mrow><mo>−</mo><mn>3</mn><mi mathvariant=\"normal\">/</mi><mn>2</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\frac{\\partial \\hat{x}_i}{\\partial \\sigma^2}=-\\frac{1}{2}(x_i - \\mu)(\\sigma^2 + \\varepsilon)^{-3/2}</annotation></semantics></math>\n",
    "Now sum over all i because variance affects every <math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mover accent=\"true\"><mi>x</mi><mo>^</mo></mover><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\hat{x}_i</annotation></semantics></math>. \n",
    "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mfrac><mrow><mi mathvariant=\"normal\">∂</mi><mi>L</mi></mrow><mrow><mi mathvariant=\"normal\">∂</mi><msup><mi>σ</mi><mn>2</mn></msup></mrow></mfrac><mo>=</mo><munder><mo>∑</mo><mi>i</mi></munder><mfrac><mrow><mi mathvariant=\"normal\">∂</mi><mi>L</mi></mrow><mrow><mi mathvariant=\"normal\">∂</mi><msub><mover accent=\"true\"><mi>x</mi><mo>^</mo></mover><mi>i</mi></msub></mrow></mfrac><mfrac><mrow><mi mathvariant=\"normal\">∂</mi><msub><mover accent=\"true\"><mi>x</mi><mo>^</mo></mover><mi>i</mi></msub></mrow><mrow><mi mathvariant=\"normal\">∂</mi><msup><mi>σ</mi><mn>2</mn></msup></mrow></mfrac></mrow><annotation encoding=\"application/x-tex\">\\frac{\\partial L}{\\partial \\sigma^2}=\\sum_i\\frac{\\partial L}{\\partial \\hat{x}_i}\\frac{\\partial \\hat{x}_i}{\\partial \\sigma^2}</annotation></semantics></math>\n",
    "\n",
    "### Gradient wrt mean\n",
    "Mean affects <math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mover accent=\"true\"><mi>x</mi><mo>^</mo></mover><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\hat{x}_i</annotation></semantics></math> in two ways:\n",
    "#### Path A — direct\n",
    "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mfrac><mrow><mi mathvariant=\"normal\">∂</mi><msub><mover accent=\"true\"><mi>x</mi><mo>^</mo></mover><mi>i</mi></msub></mrow><mrow><mi mathvariant=\"normal\">∂</mi><mi>μ</mi></mrow></mfrac><mo>=</mo><mo>−</mo><mo stretchy=\"false\">(</mo><msup><mi>σ</mi><mn>2</mn></msup><mo>+</mo><mi>ε</mi><msup><mo stretchy=\"false\">)</mo><mrow><mo>−</mo><mn>1</mn><mi mathvariant=\"normal\">/</mi><mn>2</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\frac{\\partial \\hat{x}_i}{\\partial \\mu}=-(\\sigma^2 + \\varepsilon)^{-1/2}</annotation></semantics></math>\n",
    "\n",
    "#### Path B — indirect via variance\n",
    "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mfrac><mrow><mi mathvariant=\"normal\">∂</mi><msup><mi>σ</mi><mn>2</mn></msup></mrow><mrow><mi mathvariant=\"normal\">∂</mi><mi>μ</mi></mrow></mfrac><mo>=</mo><mo>−</mo><mfrac><mn>2</mn><mrow><mi>m</mi><mo>−</mo><mn>1</mn></mrow></mfrac><munder><mo>∑</mo><mi>i</mi></munder><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>−</mo><mi>μ</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\frac{\\partial \\sigma^2}{\\partial \\mu}=-\\frac{2}{m-1}\\sum_i (x_i - \\mu)</annotation></semantics></math>\n",
    "But:\n",
    "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><munder><mo>∑</mo><mi>i</mi></munder><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>−</mo><mi>μ</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">\\sum_i (x_i - \\mu) = 0</annotation></semantics></math>\n",
    "So this entire term vanishes.\n",
    "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mfrac><mrow><mi mathvariant=\"normal\">∂</mi><mi>L</mi></mrow><mrow><mi mathvariant=\"normal\">∂</mi><mi>μ</mi></mrow></mfrac><mo>=</mo><mo>−</mo><munder><mo>∑</mo><mi>i</mi></munder><mfrac><mrow><mi mathvariant=\"normal\">∂</mi><mi>L</mi></mrow><mrow><mi mathvariant=\"normal\">∂</mi><msub><mover accent=\"true\"><mi>x</mi><mo>^</mo></mover><mi>i</mi></msub></mrow></mfrac><mo stretchy=\"false\">(</mo><msup><mi>σ</mi><mn>2</mn></msup><mo>+</mo><mi>ε</mi><msup><mo stretchy=\"false\">)</mo><mrow><mo>−</mo><mn>1</mn><mi mathvariant=\"normal\">/</mi><mn>2</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\frac{\\partial L}{\\partial \\mu}=-\\sum_i\\frac{\\partial L}{\\partial \\hat{x}_i}(\\sigma^2 + \\varepsilon)^{-1/2}</annotation></semantics></math>\n",
    "\n",
    "### Gradient wrt input\n",
    "Now we combine everything.\n",
    "#### Contribution 1 — direct path\n",
    "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mfrac><mrow><mi mathvariant=\"normal\">∂</mi><msub><mover accent=\"true\"><mi>x</mi><mo>^</mo></mover><mi>i</mi></msub></mrow><mrow><mi mathvariant=\"normal\">∂</mi><msub><mi>x</mi><mi>i</mi></msub></mrow></mfrac><mo>=</mo><mo stretchy=\"false\">(</mo><msup><mi>σ</mi><mn>2</mn></msup><mo>+</mo><mi>ε</mi><msup><mo stretchy=\"false\">)</mo><mrow><mo>−</mo><mn>1</mn><mi mathvariant=\"normal\">/</mi><mn>2</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\frac{\\partial \\hat{x}_i}{\\partial x_i}=(\\sigma^2 + \\varepsilon)^{-1/2}</annotation></semantics></math>\n",
    "\n",
    "#### Contribution 2 — via mean\n",
    "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mfrac><mrow><mi mathvariant=\"normal\">∂</mi><mi>μ</mi></mrow><mrow><mi mathvariant=\"normal\">∂</mi><msub><mi>x</mi><mi>i</mi></msub></mrow></mfrac><mo>=</mo><mfrac><mn>1</mn><mi>m</mi></mfrac></mrow><annotation encoding=\"application/x-tex\">\\frac{\\partial \\mu}{\\partial x_i}=\\frac{1}{m}</annotation></semantics></math>\n",
    "\n",
    "#### Contribution 3 — via variance\n",
    "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mfrac><mrow><mi mathvariant=\"normal\">∂</mi><msup><mi>σ</mi><mn>2</mn></msup></mrow><mrow><mi mathvariant=\"normal\">∂</mi><msub><mi>x</mi><mi>i</mi></msub></mrow></mfrac><mo>=</mo><mfrac><mn>2</mn><mrow><mi>m</mi><mo>−</mo><mn>1</mn></mrow></mfrac><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>−</mo><mi>μ</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\frac{\\partial \\sigma^2}{\\partial x_i}=\\frac{2}{m-1}(x_i - \\mu)</annotation></semantics></math>\n",
    "\n",
    "#### Putting them together:\n",
    "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><menclose notation=\"box\"><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mstyle scriptlevel=\"0\" displaystyle=\"true\"><mrow><mfrac><mrow><mi mathvariant=\"normal\">∂</mi><mi>L</mi></mrow><mrow><mi mathvariant=\"normal\">∂</mi><msub><mi>x</mi><mi>i</mi></msub></mrow></mfrac><mo>=</mo><mfrac><mrow><mi mathvariant=\"normal\">∂</mi><mi>L</mi></mrow><mrow><mi mathvariant=\"normal\">∂</mi><msub><mover accent=\"true\"><mi>x</mi><mo>^</mo></mover><mi>i</mi></msub></mrow></mfrac><mo stretchy=\"false\">(</mo><msup><mi>σ</mi><mn>2</mn></msup><mo>+</mo><mi>ε</mi><msup><mo stretchy=\"false\">)</mo><mrow><mo>−</mo><mn>1</mn><mi mathvariant=\"normal\">/</mi><mn>2</mn></mrow></msup><mo>+</mo><mfrac><mrow><mi mathvariant=\"normal\">∂</mi><mi>L</mi></mrow><mrow><mi mathvariant=\"normal\">∂</mi><msup><mi>σ</mi><mn>2</mn></msup></mrow></mfrac><mfrac><mrow><mn>2</mn><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>−</mo><mi>μ</mi><mo stretchy=\"false\">)</mo></mrow><mrow><mi>m</mi><mo>−</mo><mn>1</mn></mrow></mfrac><mo>+</mo><mfrac><mrow><mi mathvariant=\"normal\">∂</mi><mi>L</mi></mrow><mrow><mi mathvariant=\"normal\">∂</mi><mi>μ</mi></mrow></mfrac><mfrac><mn>1</mn><mi>m</mi></mfrac></mrow></mstyle></mstyle></mstyle></menclose></mrow><annotation encoding=\"application/x-tex\">\\boxed{\\frac{\\partial L}{\\partial x_i}=\\frac{\\partial L}{\\partial \\hat{x}_i}(\\sigma^2 + \\varepsilon)^{-1/2}+\\frac{\\partial L}{\\partial \\sigma^2}\\frac{2(x_i - \\mu)}{m-1}+\\frac{\\partial L}{\\partial \\mu}\\frac{1}{m}}</annotation></semantics></math>\n",
    "\n",
    "\n",
    "# Rewrite each term using BN variables\n",
    "\n",
    "### Direct term\n",
    "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mfrac><mrow><mi mathvariant=\"normal\">∂</mi><mi>L</mi></mrow><mrow><mi mathvariant=\"normal\">∂</mi><mover accent=\"true\"><mi>x</mi><mo>^</mo></mover></mrow></mfrac><mo>⋅</mo><mfrac><mn>1</mn><msqrt><mrow><msup><mi>σ</mi><mn>2</mn></msup><mo>+</mo><mi>ϵ</mi></mrow></msqrt></mfrac><mo>=</mo><mi>γ</mi><mo>⋅</mo><mi>d</mi><mi>h</mi><mi>p</mi><mi>r</mi><mi>e</mi><mi>a</mi><mi>c</mi><mi>t</mi><mo>⋅</mo><mi>b</mi><mi>n</mi><mi>v</mi><mi>a</mi><mi>r</mi><mi mathvariant=\"normal\">_</mi><mi>i</mi><mi>n</mi><mi>v</mi></mrow><annotation encoding=\"application/x-tex\">\\frac{\\partial L}{\\partial \\hat{x}} \\cdot \\frac{1}{\\sqrt{\\sigma^2+\\epsilon}}=\\gamma \\cdot dhpreact \\cdot bnvar\\_inv</annotation></semantics></math>\n",
    "\n",
    "This gives:\n",
    "bngain * bnvar_inv * dhpreact\n",
    "\n",
    "### Mean term\n",
    "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mfrac><mrow><mi mathvariant=\"normal\">∂</mi><mi>L</mi></mrow><mrow><mi mathvariant=\"normal\">∂</mi><mi>μ</mi></mrow></mfrac><mo>=</mo><mo>−</mo><munder><mo>∑</mo><mi>i</mi></munder><mfrac><mrow><mi mathvariant=\"normal\">∂</mi><mi>L</mi></mrow><mrow><mi mathvariant=\"normal\">∂</mi><msub><mover accent=\"true\"><mi>x</mi><mo>^</mo></mover><mi>i</mi></msub></mrow></mfrac><mo>⋅</mo><mi>b</mi><mi>n</mi><mi>v</mi><mi>a</mi><mi>r</mi><mi mathvariant=\"normal\">_</mi><mi>i</mi><mi>n</mi><mi>v</mi></mrow><annotation encoding=\"application/x-tex\">\\frac{\\partial L}{\\partial \\mu}=-\\sum_i \\frac{\\partial L}{\\partial \\hat{x}_i} \\cdot bnvar\\_inv</annotation></semantics></math>\n",
    "\n",
    "So:\n",
    "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mfrac><mrow><mi mathvariant=\"normal\">∂</mi><mi>L</mi></mrow><mrow><mi mathvariant=\"normal\">∂</mi><mi>μ</mi></mrow></mfrac><mo>⋅</mo><mfrac><mn>1</mn><mi>n</mi></mfrac><mo>=</mo><mo>−</mo><mfrac><mn>1</mn><mi>n</mi></mfrac><munder><mo>∑</mo><mi>i</mi></munder><mi>γ</mi><mo>⋅</mo><mi>d</mi><mi>h</mi><mi>p</mi><mi>r</mi><mi>e</mi><mi>a</mi><mi>c</mi><mi>t</mi><mo>⋅</mo><mi>b</mi><mi>n</mi><mi>v</mi><mi>a</mi><mi>r</mi><mi mathvariant=\"normal\">_</mi><mi>i</mi><mi>n</mi><mi>v</mi></mrow><annotation encoding=\"application/x-tex\">\\frac{\\partial L}{\\partial \\mu} \\cdot \\frac{1}{n}=-\\frac{1}{n} \\sum_i \\gamma \\cdot dhpreact \\cdot bnvar\\_inv</annotation></semantics></math>\n",
    "\n",
    "Vectorized:\n",
    "-bngain * bnvar_inv * dhpreact.sum(0) / n\n",
    "\n",
    "### Variance term\n",
    "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mfrac><mrow><mi mathvariant=\"normal\">∂</mi><mi>L</mi></mrow><mrow><mi mathvariant=\"normal\">∂</mi><msup><mi>σ</mi><mn>2</mn></msup></mrow></mfrac><mo>=</mo><mo>−</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><munder><mo>∑</mo><mi>i</mi></munder><mfrac><mrow><mi mathvariant=\"normal\">∂</mi><mi>L</mi></mrow><mrow><mi mathvariant=\"normal\">∂</mi><msub><mover accent=\"true\"><mi>x</mi><mo>^</mo></mover><mi>i</mi></msub></mrow></mfrac><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>−</mo><mi>μ</mi><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">(</mo><msup><mi>σ</mi><mn>2</mn></msup><mo>+</mo><mi>ϵ</mi><msup><mo stretchy=\"false\">)</mo><mrow><mo>−</mo><mn>3</mn><mi mathvariant=\"normal\">/</mi><mn>2</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\frac{\\partial L}{\\partial \\sigma^2}=-\\frac{1}{2}\\sum_i\\frac{\\partial L}{\\partial \\hat{x}_i}(x_i-\\mu)(\\sigma^2+\\epsilon)^{-3/2}</annotation></semantics></math>\n",
    "\n",
    "Recall:\n",
    "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>b</mi><mi>n</mi><mi>r</mi><mi>a</mi><mi>w</mi><mo>=</mo><mfrac><mrow><mi>x</mi><mo>−</mo><mi>μ</mi></mrow><msqrt><mrow><msup><mi>σ</mi><mn>2</mn></msup><mo>+</mo><mi>ϵ</mi></mrow></msqrt></mfrac></mrow><annotation encoding=\"application/x-tex\">bnraw = \\frac{x-\\mu}{\\sqrt{\\sigma^2+\\epsilon}}</annotation></semantics></math>\n",
    "\n",
    "So:\n",
    "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>−</mo><mi>μ</mi><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">(</mo><msup><mi>σ</mi><mn>2</mn></msup><mo>+</mo><mi>ϵ</mi><msup><mo stretchy=\"false\">)</mo><mrow><mo>−</mo><mn>3</mn><mi mathvariant=\"normal\">/</mi><mn>2</mn></mrow></msup><mo>=</mo><mi>b</mi><mi>n</mi><mi>r</mi><mi>a</mi><msub><mi>w</mi><mi>i</mi></msub><mo>⋅</mo><mi>b</mi><mi>n</mi><mi>v</mi><mi>a</mi><mi>r</mi><mi mathvariant=\"normal\">_</mi><mi>i</mi><mi>n</mi><mi>v</mi></mrow><annotation encoding=\"application/x-tex\">(x_i-\\mu)(\\sigma^2+\\epsilon)^{-3/2}=bnraw_i \\cdot bnvar\\_inv</annotation></semantics></math>\n",
    "\n",
    "Thus:\n",
    "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mfrac><mrow><mi mathvariant=\"normal\">∂</mi><mi>L</mi></mrow><mrow><mi mathvariant=\"normal\">∂</mi><msup><mi>σ</mi><mn>2</mn></msup></mrow></mfrac><mo>=</mo><mo>−</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><munder><mo>∑</mo><mi>i</mi></munder><mi>γ</mi><mo>⋅</mo><mi>d</mi><mi>h</mi><mi>p</mi><mi>r</mi><mi>e</mi><mi>a</mi><mi>c</mi><msub><mi>t</mi><mi>i</mi></msub><mo>⋅</mo><mi>b</mi><mi>n</mi><mi>r</mi><mi>a</mi><msub><mi>w</mi><mi>i</mi></msub><mo>⋅</mo><mi>b</mi><mi>n</mi><mi>v</mi><mi>a</mi><mi>r</mi><mi mathvariant=\"normal\">_</mi><mi>i</mi><mi>n</mi><mi>v</mi></mrow><annotation encoding=\"application/x-tex\">\\frac{\\partial L}{\\partial \\sigma^2}=-\\frac{1}{2} \\sum_i \\gamma \\cdot dhpreact_i \\cdot bnraw_i \\cdot bnvar\\_inv</annotation></semantics></math>\n",
    "\n",
    "Now multiply by:\n",
    "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mfrac><mrow><mn>2</mn><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>−</mo><mi>μ</mi><mo stretchy=\"false\">)</mo></mrow><mrow><mi>n</mi><mo>−</mo><mn>1</mn></mrow></mfrac></mrow><annotation encoding=\"application/x-tex\">\\frac{2(x_i-\\mu)}{n-1}</annotation></semantics></math>\n",
    "The 2 cancels the 1/2, giving:\n",
    "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mo>−</mo><mi>γ</mi><mo>⋅</mo><mi>b</mi><mi>n</mi><mi>v</mi><mi>a</mi><mi>r</mi><mi mathvariant=\"normal\">_</mi><mi>i</mi><mi>n</mi><mi>v</mi><mo>⋅</mo><mfrac><mi>n</mi><mrow><mi>n</mi><mo>−</mo><mn>1</mn></mrow></mfrac><mo>⋅</mo><mi>b</mi><mi>n</mi><mi>r</mi><mi>a</mi><msub><mi>w</mi><mi>i</mi></msub><munder><mo>∑</mo><mi>j</mi></munder><mi>d</mi><mi>h</mi><mi>p</mi><mi>r</mi><mi>e</mi><mi>a</mi><mi>c</mi><msub><mi>t</mi><mi>j</mi></msub><mo>⋅</mo><mi>b</mi><mi>n</mi><mi>r</mi><mi>a</mi><msub><mi>w</mi><mi>j</mi></msub></mrow><annotation encoding=\"application/x-tex\">- \\gamma \\cdot bnvar\\_inv \\cdot \\frac{n}{n-1} \\cdot bnraw_i \\sum_j dhpreact_j \\cdot bnraw_j</annotation></semantics></math>\n",
    "\n",
    "Vectorized:\n",
    "-bngain * bnvar_inv * n/(n-1) * bnraw * (dhpreact * bnraw).sum(0)\n",
    "\n",
    "### Combine all three terms\n",
    "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mfrac><mrow><mi mathvariant=\"normal\">∂</mi><mi>L</mi></mrow><mrow><mi mathvariant=\"normal\">∂</mi><mi>x</mi></mrow></mfrac><mo>=</mo><mi>γ</mi><mo>⋅</mo><mi>b</mi><mi>n</mi><mi>v</mi><mi>a</mi><mi>r</mi><mi mathvariant=\"normal\">_</mi><mi>i</mi><mi>n</mi><mi>v</mi><mo fence=\"false\" stretchy=\"true\" minsize=\"1.8em\" maxsize=\"1.8em\">(</mo><mi>d</mi><mi>h</mi><mi>p</mi><mi>r</mi><mi>e</mi><mi>a</mi><mi>c</mi><mi>t</mi><mo>−</mo><mfrac><mn>1</mn><mi>n</mi></mfrac><mi>d</mi><mi>h</mi><mi>p</mi><mi>r</mi><mi>e</mi><mi>a</mi><mi>c</mi><mi>t</mi><mi mathvariant=\"normal\">.</mi><mi>s</mi><mi>u</mi><mi>m</mi><mo stretchy=\"false\">(</mo><mn>0</mn><mo stretchy=\"false\">)</mo><mo>−</mo><mfrac><mi>n</mi><mrow><mi>n</mi><mo>−</mo><mn>1</mn></mrow></mfrac><mi>b</mi><mi>n</mi><mi>r</mi><mi>a</mi><mi>w</mi><mo stretchy=\"false\">(</mo><mi>d</mi><mi>h</mi><mi>p</mi><mi>r</mi><mi>e</mi><mi>a</mi><mi>c</mi><mi>t</mi><mo>⋅</mo><mi>b</mi><mi>n</mi><mi>r</mi><mi>a</mi><mi>w</mi><mo stretchy=\"false\">)</mo><mi mathvariant=\"normal\">.</mi><mi>s</mi><mi>u</mi><mi>m</mi><mo stretchy=\"false\">(</mo><mn>0</mn><mo stretchy=\"false\">)</mo><mo fence=\"false\" stretchy=\"true\" minsize=\"1.8em\" maxsize=\"1.8em\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\frac{\\partial L}{\\partial x}=\\gamma \\cdot bnvar\\_inv\\Big(dhpreact- \\frac{1}{n} dhpreact.sum(0)- \\frac{n}{n-1} bnraw (dhpreact \\cdot bnraw).sum(0)\\Big)</annotation></semantics></math>\n",
    "\n",
    "\n",
    "Factor out 1/n:\n",
    "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mo>=</mo><mi>γ</mi><mo>⋅</mo><mi>b</mi><mi>n</mi><mi>v</mi><mi>a</mi><mi>r</mi><mi mathvariant=\"normal\">_</mi><mi>i</mi><mi>n</mi><mi>v</mi><mo>⋅</mo><mfrac><mn>1</mn><mi>n</mi></mfrac><mo fence=\"false\" stretchy=\"true\" minsize=\"1.8em\" maxsize=\"1.8em\">(</mo><mi>n</mi><mo>⋅</mo><mi>d</mi><mi>h</mi><mi>p</mi><mi>r</mi><mi>e</mi><mi>a</mi><mi>c</mi><mi>t</mi><mo>−</mo><mi>d</mi><mi>h</mi><mi>p</mi><mi>r</mi><mi>e</mi><mi>a</mi><mi>c</mi><mi>t</mi><mi mathvariant=\"normal\">.</mi><mi>s</mi><mi>u</mi><mi>m</mi><mo stretchy=\"false\">(</mo><mn>0</mn><mo stretchy=\"false\">)</mo><mo>−</mo><mfrac><mi>n</mi><mrow><mi>n</mi><mo>−</mo><mn>1</mn></mrow></mfrac><mi>b</mi><mi>n</mi><mi>r</mi><mi>a</mi><mi>w</mi><mo stretchy=\"false\">(</mo><mi>d</mi><mi>h</mi><mi>p</mi><mi>r</mi><mi>e</mi><mi>a</mi><mi>c</mi><mi>t</mi><mo>⋅</mo><mi>b</mi><mi>n</mi><mi>r</mi><mi>a</mi><mi>w</mi><mo stretchy=\"false\">)</mo><mi mathvariant=\"normal\">.</mi><mi>s</mi><mi>u</mi><mi>m</mi><mo stretchy=\"false\">(</mo><mn>0</mn><mo stretchy=\"false\">)</mo><mo fence=\"false\" stretchy=\"true\" minsize=\"1.8em\" maxsize=\"1.8em\">)</mo></mrow><annotation encoding=\"application/x-tex\">=\\gamma \\cdot bnvar\\_inv \\cdot \\frac{1}{n}\\Big(n \\cdot dhpreact- dhpreact.sum(0)- \\frac{n}{n-1} bnraw (dhpreact \\cdot bnraw).sum(0)\\Big)</annotation></semantics></math>\n",
    "\n",
    "This is exactly:\n",
    "dhprebn = (\n",
    "    bngain\n",
    "    * bnvar_inv\n",
    "    / n\n",
    "    * (n * dhpreact\n",
    "       - dhpreact.sum(0)\n",
    "       - n / (n - 1) * bnraw * (dhpreact * bnraw).sum(0))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12105f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Batch Normalization (per-feature, training mode)\n",
    "# Mathematical + Python explanation\n",
    "# ============================================================\n",
    "\n",
    "# Given a batch x of size m (shape: [m])\n",
    "# Forward definitions (from the board):\n",
    "\n",
    "# μ = (1/m) * sum_i x_i\n",
    "# σ² = (1/(m-1)) * sum_i (x_i - μ)²      # Bessel correction (as in your notes)\n",
    "# x̂_i = (x_i - μ) / sqrt(σ² + ε)\n",
    "# y_i = γ * x̂_i + β\n",
    "\n",
    "# We assume upstream gradient is given:\n",
    "# dL/dy_i  (from later layers)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# FORWARD PASS (what happens numerically)\n",
    "# ============================================================\n",
    "\n",
    "# m = x.shape[0]\n",
    "\n",
    "# mu = x.mean(dim=0)  # μ\n",
    "# var = ((x - mu) ** 2).sum(dim=0) / (m - 1)  # σ²\n",
    "# inv_std = (var + eps) ** -0.5  # (σ² + ε)^(-1/2)\n",
    "\n",
    "# x_hat = (x - mu) * inv_std  # x̂\n",
    "# y = gamma * x_hat + beta  # BN output\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# BACKWARD PASS (this is what the board derives)\n",
    "# ============================================================\n",
    "\n",
    "# We want: dL/dx_i\n",
    "# But x_i influences L through THREE paths:\n",
    "#\n",
    "# 1) x_i → x̂_i → y_i → L\n",
    "# 2) x_i → μ → x̂_j → y_j → L  (affects all j)\n",
    "# 3) x_i → σ² → x̂_j → y_j → L (affects all j)\n",
    "#\n",
    "# Chain rule:\n",
    "#\n",
    "# dL/dx_i =\n",
    "#   (dL/dx̂_i)(dx̂_i/dx_i)\n",
    "# + (dL/dμ)(dμ/dx_i)\n",
    "# + (dL/dσ²)(dσ²/dx_i)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Step 1: gradient wrt affine params γ and β\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# From y_i = γ x̂_i + β\n",
    "\n",
    "# dbeta = dL_dy.sum(dim=0)  # ∂L/∂β\n",
    "# dgamma = (dL_dy * x_hat).sum(dim=0)  # ∂L/∂γ\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Step 2: gradient wrt normalized activations x̂\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# ∂L/∂x̂_i = ∂L/∂y_i * γ\n",
    "# dL_dxhat = dL_dy * gamma\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Step 3: gradient wrt variance σ²\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# x̂_i = (x_i - μ)(σ² + ε)^(-1/2)\n",
    "#\n",
    "# ∂x̂_i/∂σ² = -1/2 * (x_i - μ) * (σ² + ε)^(-3/2)\n",
    "\n",
    "# dL_dvar = (-0.5 * (dL_dxhat * (x - mu)) * (var + eps) ** -1.5).sum(dim=0)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Step 4: gradient wrt mean μ\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Two contributions:\n",
    "#\n",
    "# from x̂_i term:\n",
    "#   ∂x̂_i/∂μ = -(σ² + ε)^(-1/2)\n",
    "#\n",
    "# from variance term:\n",
    "#   ∂σ²/∂μ = -2/(m-1) * sum_i (x_i - μ)\n",
    "\n",
    "# dL_dmu = -(dL_dxhat * inv_std).sum(dim=0) + dL_dvar * (-2 / (m - 1)) * (x - mu).sum(\n",
    "#     dim=0\n",
    "# )\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Step 5: gradient wrt input x\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Combine all three paths:\n",
    "#\n",
    "# ∂x̂_i/∂x_i = (σ² + ε)^(-1/2)\n",
    "# ∂μ/∂x_i = 1/m\n",
    "# ∂σ²/∂x_i = 2/(m-1) * (x_i - μ)\n",
    "\n",
    "# dx = dL_dxhat * inv_std + dL_dvar * (2 / (m - 1)) * (x - mu) + dL_dmu / m\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# FINAL INTUITION (important)\n",
    "# ============================================================\n",
    "\n",
    "# 1) BatchNorm backward is hard because x affects:\n",
    "#    - its own normalized value\n",
    "#    - the batch mean\n",
    "#    - the batch variance\n",
    "#\n",
    "# 2) Every input influences every other input through μ and σ²\n",
    "#\n",
    "# 3) The ugly algebra simplifies to this structured flow:\n",
    "#\n",
    "#   dL/dx =\n",
    "#     local term\n",
    "#   + variance correction\n",
    "#   + mean correction\n",
    "#\n",
    "# 4) PyTorch autograd does ALL of this for you automatically.\n",
    "#    This derivation explains *why it works*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333b49ef",
   "metadata": {},
   "source": [
    "# putting it all together!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e608e73a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12297\n",
      "      0/ 200000: 3.7903\n",
      "  10000/ 200000: 2.1986\n",
      "  20000/ 200000: 2.3505\n",
      "  30000/ 200000: 2.4042\n",
      "  40000/ 200000: 1.9833\n",
      "  50000/ 200000: 2.4029\n",
      "  60000/ 200000: 2.4053\n",
      "  70000/ 200000: 2.0550\n",
      "  80000/ 200000: 2.3601\n",
      "  90000/ 200000: 2.2228\n",
      " 100000/ 200000: 1.9131\n",
      " 110000/ 200000: 2.3941\n",
      " 120000/ 200000: 1.9764\n",
      " 130000/ 200000: 2.4240\n",
      " 140000/ 200000: 2.2891\n",
      " 150000/ 200000: 2.1337\n",
      " 160000/ 200000: 1.9630\n",
      " 170000/ 200000: 1.8412\n",
      " 180000/ 200000: 1.9666\n",
      " 190000/ 200000: 1.8865\n"
     ]
    }
   ],
   "source": [
    "# Exercise 4: putting it all together!\n",
    "# Train the MLP neural net with your own backward pass\n",
    "\n",
    "# init\n",
    "n_embd = 10  # the dimensionality of the character embedding vectors\n",
    "n_hidden = 200  # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)  # for reproducibility\n",
    "C = torch.randn((vocab_size, n_embd), generator=g)\n",
    "# Layer 1\n",
    "W1 = (\n",
    "    torch.randn((n_embd * block_size, n_hidden), generator=g)\n",
    "    * (5 / 3)\n",
    "    / ((n_embd * block_size) ** 0.5)\n",
    ")\n",
    "b1 = torch.randn(n_hidden, generator=g) * 0.1\n",
    "# Layer 2\n",
    "W2 = torch.randn((n_hidden, vocab_size), generator=g) * 0.1\n",
    "b2 = torch.randn(vocab_size, generator=g) * 0.1\n",
    "# BatchNorm parameters\n",
    "bngain = torch.randn((1, n_hidden)) * 0.1 + 1.0\n",
    "bnbias = torch.randn((1, n_hidden)) * 0.1\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters))  # number of parameters in total\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "# same optimization as last time\n",
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "n = batch_size  # convenience\n",
    "lossi = []\n",
    "\n",
    "# use this context manager for efficiency once your backward pass is written (TODO)\n",
    "with torch.no_grad():\n",
    "\n",
    "    # kick off optimization\n",
    "    for i in range(max_steps):\n",
    "\n",
    "        # minibatch construct\n",
    "        ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "        Xb, Yb = Xtr[ix], Ytr[ix]  # batch X,Y\n",
    "\n",
    "        # forward pass\n",
    "        emb = C[Xb]  # embed the characters into vectors\n",
    "        embcat = emb.view(emb.shape[0], -1)  # concatenate the vectors\n",
    "        # Linear layer\n",
    "        hprebn = embcat @ W1 + b1  # hidden layer pre-activation\n",
    "        # BatchNorm layer\n",
    "        # -------------------------------------------------------------\n",
    "        bnmean = hprebn.mean(0, keepdim=True)\n",
    "        bnvar = hprebn.var(0, keepdim=True, unbiased=True)\n",
    "        bnvar_inv = (bnvar + 1e-5) ** -0.5\n",
    "        bnraw = (hprebn - bnmean) * bnvar_inv\n",
    "        hpreact = bngain * bnraw + bnbias\n",
    "        # -------------------------------------------------------------\n",
    "        # Non-linearity\n",
    "        h = torch.tanh(hpreact)  # hidden layer\n",
    "        logits = h @ W2 + b2  # output layer\n",
    "        loss = F.cross_entropy(logits, Yb)  # loss function\n",
    "\n",
    "        # backward pass\n",
    "        for p in parameters:\n",
    "            p.grad = None\n",
    "        #loss.backward() # use this for correctness comparisons, delete it later!\n",
    "\n",
    "        # manual backprop! #swole_doge_meme\n",
    "        # -----------------\n",
    "        dlogits = F.softmax(logits, 1)\n",
    "        dlogits[range(n), Yb] -= 1\n",
    "        dlogits /= n\n",
    "        # 2nd layer backprop\n",
    "        dh = dlogits @ W2.T\n",
    "        dW2 = h.T @ dlogits\n",
    "        db2 = dlogits.sum(0)\n",
    "        # tanh\n",
    "        dhpreact = (1.0 - h**2) * dh\n",
    "        # batchnorm backprop\n",
    "        dbngain = (bnraw * dhpreact).sum(0, keepdim=True)\n",
    "        dbnbias = dhpreact.sum(0, keepdim=True)\n",
    "        dhprebn = (\n",
    "            bngain\n",
    "            * bnvar_inv\n",
    "            / n\n",
    "            * (\n",
    "                n * dhpreact\n",
    "                - dhpreact.sum(0)\n",
    "                - n / (n - 1) * bnraw * (dhpreact * bnraw).sum(0)\n",
    "            )\n",
    "        )\n",
    "        # 1st layer\n",
    "        dembcat = dhprebn @ W1.T\n",
    "        dW1 = embcat.T @ dhprebn\n",
    "        db1 = dhprebn.sum(0)\n",
    "        # embedding\n",
    "        demb = dembcat.view(emb.shape)\n",
    "        dC = torch.zeros_like(C)\n",
    "        for k in range(Xb.shape[0]):\n",
    "            for j in range(Xb.shape[1]):\n",
    "                ix = Xb[k, j]\n",
    "                dC[ix] += demb[k, j]\n",
    "        grads = [dC, dW1, db1, dW2, db2, dbngain, dbnbias]\n",
    "        # -----------------\n",
    "\n",
    "        # update\n",
    "        lr = 0.1 if i < 100000 else 0.01  # step learning rate decay\n",
    "        for p, grad in zip(parameters, grads):\n",
    "            # p.data += -lr * p.grad # old way of cheems doge (using PyTorch grad from .backward())\n",
    "            p.data += -lr * grad  # new way of swole doge TODO: enable\n",
    "\n",
    "        # track stats\n",
    "        if i % 10000 == 0:  # print every once in a while\n",
    "            print(f\"{i:7d}/{max_steps:7d}: {loss.item():.4f}\")\n",
    "        lossi.append(loss.log10().item())\n",
    "\n",
    "    #   if i >= 100: # TODO: delete early breaking when you're ready to train the full net\n",
    "    #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a9fd0f5b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "all() received an invalid combination of arguments - got (bool), but expected one of:\n * (Tensor input, *, Tensor out)\n * (Tensor input, tuple of ints dim, bool keepdim, *, Tensor out)\n * (Tensor input, int dim, bool keepdim, *, Tensor out)\n * (Tensor input, name dim, bool keepdim, *, Tensor out)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[96], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# useful for checking your gradients\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p,g \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(parameters, grads):\n\u001b[0;32m----> 3\u001b[0m   \u001b[43mcmp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[33], line 3\u001b[0m, in \u001b[0;36mcmp\u001b[0;34m(s, dt, t)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcmp\u001b[39m(s, dt, t):\n\u001b[0;32m----> 3\u001b[0m     ex \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m      4\u001b[0m     app \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mallclose(dt, t\u001b[38;5;241m.\u001b[39mgrad)\n\u001b[1;32m      5\u001b[0m     maxdiff \u001b[38;5;241m=\u001b[39m (dt \u001b[38;5;241m-\u001b[39m t\u001b[38;5;241m.\u001b[39mgrad)\u001b[38;5;241m.\u001b[39mabs()\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[0;31mTypeError\u001b[0m: all() received an invalid combination of arguments - got (bool), but expected one of:\n * (Tensor input, *, Tensor out)\n * (Tensor input, tuple of ints dim, bool keepdim, *, Tensor out)\n * (Tensor input, int dim, bool keepdim, *, Tensor out)\n * (Tensor input, name dim, bool keepdim, *, Tensor out)\n"
     ]
    }
   ],
   "source": [
    "# useful for checking your gradients\n",
    "for p,g in zip(parameters, grads):\n",
    "  cmp(str(tuple(p.shape)), g, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790a1418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calibrate the batch norm at the end of training\n",
    "\n",
    "with torch.no_grad():\n",
    "    # pass the training set through\n",
    "    emb = C[Xtr]\n",
    "    embcat = emb.view(emb.shape[0], -1)\n",
    "    hpreact = embcat @ W1 + b1\n",
    "    # measure the mean/std over the entire training set\n",
    "    bnmean = hpreact.mean(0, keepdim=True)\n",
    "    bnvar = hpreact.var(0, keepdim=True, unbiased=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b795e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate train and val loss\n",
    "\n",
    "\n",
    "@torch.no_grad()  # this decorator disables gradient tracking\n",
    "def split_loss(split):\n",
    "    x, y = {\n",
    "        \"train\": (Xtr, Ytr),\n",
    "        \"val\": (Xdev, Ydev),\n",
    "        \"test\": (Xte, Yte),\n",
    "    }[split]\n",
    "    emb = C[x]  # (N, block_size, n_embd)\n",
    "    embcat = emb.view(emb.shape[0], -1)  # concat into (N, block_size * n_embd)\n",
    "    hpreact = embcat @ W1 + b1\n",
    "    hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5) ** -0.5 + bnbias\n",
    "    h = torch.tanh(hpreact)  # (N, n_hidden)\n",
    "    logits = h @ W2 + b2  # (N, vocab_size)\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    print(split, loss.item())\n",
    "\n",
    "\n",
    "split_loss(\"train\")\n",
    "split_loss(\"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb80f800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample from the model\n",
    "g = torch.Generator().manual_seed(2147483647 + 10)\n",
    "\n",
    "for _ in range(20):\n",
    "\n",
    "    out = []\n",
    "    context = [0] * block_size  # initialize with all ...\n",
    "    while True:\n",
    "        # ------------\n",
    "        # forward pass:\n",
    "        # Embedding\n",
    "        emb = C[torch.tensor([context])]  # (1,block_size,d)\n",
    "        embcat = emb.view(emb.shape[0], -1)  # concat into (N, block_size * n_embd)\n",
    "        hpreact = embcat @ W1 + b1\n",
    "        hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5) ** -0.5 + bnbias\n",
    "        h = torch.tanh(hpreact)  # (N, n_hidden)\n",
    "        logits = h @ W2 + b2  # (N, vocab_size)\n",
    "        # ------------\n",
    "        # Sample\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "        context = context[1:] + [ix]\n",
    "        out.append(ix)\n",
    "        if ix == 0:\n",
    "            break\n",
    "\n",
    "    print(\"\".join(itos[i] for i in out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fcc54d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.9.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
